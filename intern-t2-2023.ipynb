{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset to dataframe\n",
    "\n",
    "Note that the dataset might be coming from BigQuery or from a query on the local database (created from the SE Data Dump). The two data sources should be interchangeable in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved copy of dataset loaded from local disk.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATASET_FILE = \"saved_dataset.csv\"      # The file name of the saved dataset (saved on / loaded from local disk)\n",
    "cwd = Path().absolute()                 # Current working directory (note: possibly different from execution directory)\n",
    "\n",
    "# Load a saved copy of the dataset from local disk (if it exists)\n",
    "try:\n",
    "    dataset_path = os.path.join(cwd, DATASET_FILE)\n",
    "    results = pd.read_csv(dataset_path)\n",
    "    results = results.astype({\"creation_date\": \"datetime64[ns]\"})\n",
    "    print(\"Saved copy of dataset loaded from local disk.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Saved dataset not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cull / filter dataset\n",
    "\n",
    "For the demo we're just arbitrarily culling the size of the dataset to make it more manageable, but you could also filter for other reasons such as focusing on a specific tag, or sampling based on answer and/or question scores.\n",
    "\n",
    "A pandas dataframe can be sampled either:\n",
    "* Using a fractional value: e.g., ``.sample(frac=0.01)`` will result in a number of samples equivalent to 1% of the dataset.\n",
    "* Using an integer value: e.g., ``.sample(n=1000)`` will result in 1000 samples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions in currently selected filtered dataset: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>accepted_answer_id</th>\n",
       "      <th>view_count</th>\n",
       "      <th>tags</th>\n",
       "      <th>answer_count</th>\n",
       "      <th>question_score</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>answer_score</th>\n",
       "      <th>stackoverflow_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>166728</th>\n",
       "      <td>70703290</td>\n",
       "      <td>change event handler in angular not reading op...</td>\n",
       "      <td>&lt;p&gt;I am trying to get the onPageSizeUpdate fun...</td>\n",
       "      <td>70703621</td>\n",
       "      <td>96</td>\n",
       "      <td>&lt;angular&gt;&lt;typescript&gt;&lt;event-handling&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-13 21:25:46</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;p&gt;Instead of&lt;/p&gt;&amp;#xA;&lt;pre&gt;&lt;code&gt;onPageSizeUpd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113902</th>\n",
       "      <td>70287406</td>\n",
       "      <td>how to replace all accented characters with En...</td>\n",
       "      <td>&lt;p&gt;Hi in my aura component below code is used ...</td>\n",
       "      <td>70288180</td>\n",
       "      <td>3194</td>\n",
       "      <td>&lt;javascript&gt;&lt;aura.js&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-12-09 09:23:19</td>\n",
       "      <td>6</td>\n",
       "      <td>&lt;pre class=\"lang-js prettyprint-override\"&gt;&lt;cod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320689</th>\n",
       "      <td>71940097</td>\n",
       "      <td>Why are React defaultProps not passing values?</td>\n",
       "      <td>&lt;pre class=\"lang-js prettyprint-override\"&gt;&lt;cod...</td>\n",
       "      <td>71940260</td>\n",
       "      <td>473</td>\n",
       "      <td>&lt;css&gt;&lt;reactjs&gt;&lt;react-memo&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-04-20 12:58:22</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;pre&gt;&lt;code&gt;Button.defaultProps = {&amp;#xA;  size:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621929</th>\n",
       "      <td>74478463</td>\n",
       "      <td>Deploy sql workflow with DBX</td>\n",
       "      <td>&lt;p&gt;I am developing deployment via DBX to Azure...</td>\n",
       "      <td>74488928</td>\n",
       "      <td>315</td>\n",
       "      <td>&lt;sql&gt;&lt;databricks&gt;&lt;databricks-dbx&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-11-17 15:56:55</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;p&gt;There are various ways to do that.&lt;/p&gt;&amp;#xA;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275583</th>\n",
       "      <td>71571109</td>\n",
       "      <td>How to write multiple excel files with multipl...</td>\n",
       "      <td>&lt;p&gt;I would like to split a data frame in order...</td>\n",
       "      <td>71623188</td>\n",
       "      <td>893</td>\n",
       "      <td>&lt;r&gt;&lt;tidyverse&gt;&lt;purrr&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-03-22 11:13:23</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;p&gt;In your example, you are using the same fil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                              title  \\\n",
       "166728  70703290  change event handler in angular not reading op...   \n",
       "113902  70287406  how to replace all accented characters with En...   \n",
       "320689  71940097     Why are React defaultProps not passing values?   \n",
       "621929  74478463                       Deploy sql workflow with DBX   \n",
       "275583  71571109  How to write multiple excel files with multipl...   \n",
       "\n",
       "                                                     body  accepted_answer_id  \\\n",
       "166728  <p>I am trying to get the onPageSizeUpdate fun...            70703621   \n",
       "113902  <p>Hi in my aura component below code is used ...            70288180   \n",
       "320689  <pre class=\"lang-js prettyprint-override\"><cod...            71940260   \n",
       "621929  <p>I am developing deployment via DBX to Azure...            74488928   \n",
       "275583  <p>I would like to split a data frame in order...            71623188   \n",
       "\n",
       "        view_count                                   tags  answer_count  \\\n",
       "166728          96  <angular><typescript><event-handling>             2   \n",
       "113902        3194                  <javascript><aura.js>             2   \n",
       "320689         473             <css><reactjs><react-memo>             1   \n",
       "621929         315      <sql><databricks><databricks-dbx>             2   \n",
       "275583         893                  <r><tidyverse><purrr>             1   \n",
       "\n",
       "        question_score       creation_date  answer_score  \\\n",
       "166728               0 2022-01-13 21:25:46             0   \n",
       "113902               1 2021-12-09 09:23:19             6   \n",
       "320689               0 2022-04-20 12:58:22             0   \n",
       "621929               0 2022-11-17 15:56:55             1   \n",
       "275583               0 2022-03-22 11:13:23             1   \n",
       "\n",
       "                                     stackoverflow_answer  \n",
       "166728  <p>Instead of</p>&#xA;<pre><code>onPageSizeUpd...  \n",
       "113902  <pre class=\"lang-js prettyprint-override\"><cod...  \n",
       "320689  <pre><code>Button.defaultProps = {&#xA;  size:...  \n",
       "621929  <p>There are various ways to do that.</p>&#xA;...  \n",
       "275583  <p>In your example, you are using the same fil...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly sample dataset\n",
    "fd_tiny = results.sample(frac=0.0001)\n",
    "fd_nano = results.sample(n=10)\n",
    "fd_1k   = results.sample(n=1000)\n",
    "fd_10k  = results.sample(n=10000)\n",
    "fd_100k = results.sample(n=100000)\n",
    "\n",
    "# Convenience: alias the filtered data so we can change it easily for later code\n",
    "wd = fd_nano\n",
    "\n",
    "# Dump info about the filtered result\n",
    "print(\"Number of questions in currently selected filtered dataset:\", len(wd))\n",
    "wd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strip HTML from filtered dataset\n",
    "\n",
    "We decided we would strip HTML and use this \"stripped\" version as our default for evaluations. The stripped text is appended as a separate column in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "# Separate out the text columns we want for convenience\n",
    "titles  = wd[\"title\"]\n",
    "bodies  = wd[\"body\"]\n",
    "answers = wd[\"stackoverflow_answer\"]\n",
    "\n",
    "# Sanity check (surely this will always be true, but *just in case*)\n",
    "if len(titles) == len(bodies) and len(titles) == len(answers):\n",
    "    pass\n",
    "else:\n",
    "    raise ValueError(\"columns are different lengths!\")\n",
    "\n",
    "# Create new lists to store HTML-stripped versions of text\n",
    "s_titles  = []\n",
    "s_bodies  = []\n",
    "s_answers = []\n",
    "\n",
    "# Iterating is slow(!) but comparatively easy to understand (modified Harvey approach)\n",
    "for idx, *row in wd.itertuples():\n",
    "    \n",
    "    # Strip HTML from title, question, and answer; save to lists\n",
    "    s_titles.append(soup(titles[idx], \"html.parser\").get_text())\n",
    "    s_bodies.append(soup(bodies[idx], \"html.parser\").get_text())\n",
    "    s_answers.append(soup(answers[idx], \"html.parser\").get_text())\n",
    "\n",
    "# Add the populated lists into our dataframe\n",
    "wd[\"stripped_title\"] = s_titles\n",
    "wd[\"stripped_body\"] = s_bodies\n",
    "wd[\"stripped_stackoverflow_answer\"] = s_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install OpenAI library and Configure OpenAI API Key\n",
    "\n",
    "Currently configured using secrets.json located at the root directory. An alternative method (which would require code changes) would be to read the system's environment variable.\n",
    "\n",
    "Key can be generated from: https://platform.openai.com/account/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (0.28.0)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from openai) (3.8.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from requests>=2.20->openai) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from requests>=2.20->openai) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from requests>=2.20->openai) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from aiohttp->openai) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from tqdm->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install OpenAI\n",
    "%pip install openai\n",
    "import openai\n",
    "\n",
    "# Function to load OpenAI API key from file\n",
    "# https://stackoverflow.com/a/76148268\n",
    "def load_api_key(secrets_file=\"secrets.json\"):\n",
    "    with open(secrets_file) as f:\n",
    "        secrets = json.load(f)\n",
    "    return secrets[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Read and set our OpenAI API key\n",
    "api_key = load_api_key()\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataframe into chunks for batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>accepted_answer_id</th>\n",
       "      <th>view_count</th>\n",
       "      <th>tags</th>\n",
       "      <th>answer_count</th>\n",
       "      <th>question_score</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>answer_score</th>\n",
       "      <th>stackoverflow_answer</th>\n",
       "      <th>stripped_title</th>\n",
       "      <th>stripped_body</th>\n",
       "      <th>stripped_stackoverflow_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>166728</th>\n",
       "      <td>70703290</td>\n",
       "      <td>change event handler in angular not reading op...</td>\n",
       "      <td>&lt;p&gt;I am trying to get the onPageSizeUpdate fun...</td>\n",
       "      <td>70703621</td>\n",
       "      <td>96</td>\n",
       "      <td>&lt;angular&gt;&lt;typescript&gt;&lt;event-handling&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-13 21:25:46</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;p&gt;Instead of&lt;/p&gt;&amp;#xA;&lt;pre&gt;&lt;code&gt;onPageSizeUpd...</td>\n",
       "      <td>change event handler in angular not reading op...</td>\n",
       "      <td>I am trying to get the onPageSizeUpdate functi...</td>\n",
       "      <td>Instead of\\nonPageSizeUpdate($any($event.targe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                              title  \\\n",
       "166728  70703290  change event handler in angular not reading op...   \n",
       "\n",
       "                                                     body  accepted_answer_id  \\\n",
       "166728  <p>I am trying to get the onPageSizeUpdate fun...            70703621   \n",
       "\n",
       "        view_count                                   tags  answer_count  \\\n",
       "166728          96  <angular><typescript><event-handling>             2   \n",
       "\n",
       "        question_score       creation_date  answer_score  \\\n",
       "166728               0 2022-01-13 21:25:46             0   \n",
       "\n",
       "                                     stackoverflow_answer  \\\n",
       "166728  <p>Instead of</p>&#xA;<pre><code>onPageSizeUpd...   \n",
       "\n",
       "                                           stripped_title  \\\n",
       "166728  change event handler in angular not reading op...   \n",
       "\n",
       "                                            stripped_body  \\\n",
       "166728  I am trying to get the onPageSizeUpdate functi...   \n",
       "\n",
       "                            stripped_stackoverflow_answer  \n",
       "166728  Instead of\\nonPageSizeUpdate($any($event.targe...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Split into n parts (in our case 10) in a list\n",
    "wd_list = np.array_split(wd, 10)\n",
    "\n",
    "# Check the result of the first chunk\n",
    "wd_list[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get GPT answers to SO questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tenacity in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (8.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Used for rate limit handling with OpenAI API\n",
    "%pip install tenacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from tiktoken) (2023.8.8)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Used to estimate token counts\n",
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff\n",
    "\n",
    "MODEL_NAME = \"gpt-4\"\n",
    "ENCODING = tiktoken.encoding_for_model(MODEL_NAME)\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def completion_with_backoff(**kwargs):\n",
    "    response = openai.ChatCompletion.create(**kwargs)\n",
    "    return response\n",
    "\n",
    "def chat_format(question):\n",
    "    \"\"\"Insert the full prompt into chat format.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "skipped = 0\n",
    "\n",
    "for chunk in wd_list:\n",
    "    GPT_answers = []\n",
    "    GPT_finished = []\n",
    "    full_responses = []\n",
    "\n",
    "    for idx, row in chunk.iterrows():\n",
    "\n",
    "        title_and_question = row[\"stripped_title\"] + \"\\n\\n\" + row[\"stripped_body\"]\n",
    "        SO_text = title_and_question + row[\"stripped_stackoverflow_answer\"]\n",
    "\n",
    "        # Estimate tokens for SO T+Q+A, and skip anything too long\n",
    "        if len(ENCODING.encode(SO_text)) < 4000:\n",
    "\n",
    "            # Get the response from GPT\n",
    "            prompt = chat_format(title_and_question)\n",
    "            GPT_answer = completion_with_backoff(model=MODEL_NAME, messages=prompt, temperature=0, max_tokens=2000)\n",
    "            extracted_answer = GPT_answer.choices[0].message.content\n",
    "\n",
    "            # Check if the GPT response completed or terminated early (because e.g. hit token limit)\n",
    "            if GPT_answer.choices[0].finish_reason == \"stop\":\n",
    "                finished = True\n",
    "            else:\n",
    "                finished = False\n",
    "\n",
    "        else:\n",
    "            skipped += 1\n",
    "            extracted_answer = None\n",
    "            GPT_answer = None\n",
    "            finished = False\n",
    "\n",
    "        # Add to our lists\n",
    "        GPT_answers.append(extracted_answer)\n",
    "        GPT_finished.append(finished)\n",
    "        full_responses.append(GPT_answer)\n",
    "\n",
    "    # Add answers back into the chunk dataframe\n",
    "    COL_NAME = f\"{MODEL_NAME}_answer\"\n",
    "    chunk[COL_NAME] = GPT_answers\n",
    "    chunk[\"GPT_finished\"] = GPT_finished\n",
    "    chunk[\"full_GPT_response\"] = full_responses\n",
    "\n",
    "# Note how many questions have been skipped for length reasons\n",
    "print(skipped, \"entries were skipped due to token length.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>accepted_answer_id</th>\n",
       "      <th>view_count</th>\n",
       "      <th>tags</th>\n",
       "      <th>answer_count</th>\n",
       "      <th>question_score</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>answer_score</th>\n",
       "      <th>stackoverflow_answer</th>\n",
       "      <th>stripped_title</th>\n",
       "      <th>stripped_body</th>\n",
       "      <th>stripped_stackoverflow_answer</th>\n",
       "      <th>gpt-4_answer</th>\n",
       "      <th>GPT_finished</th>\n",
       "      <th>full_GPT_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>166728</th>\n",
       "      <td>70703290</td>\n",
       "      <td>change event handler in angular not reading op...</td>\n",
       "      <td>&lt;p&gt;I am trying to get the onPageSizeUpdate fun...</td>\n",
       "      <td>70703621</td>\n",
       "      <td>96</td>\n",
       "      <td>&lt;angular&gt;&lt;typescript&gt;&lt;event-handling&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-13 21:25:46</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;p&gt;Instead of&lt;/p&gt;&amp;#xA;&lt;pre&gt;&lt;code&gt;onPageSizeUpd...</td>\n",
       "      <td>change event handler in angular not reading op...</td>\n",
       "      <td>I am trying to get the onPageSizeUpdate functi...</td>\n",
       "      <td>Instead of\\nonPageSizeUpdate($any($event.targe...</td>\n",
       "      <td>The issue is that you are not setting the valu...</td>\n",
       "      <td>True</td>\n",
       "      <td>{'id': 'chatcmpl-83JxCm9GK225VdHnBpjhwEUovO6dc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                              title  \\\n",
       "166728  70703290  change event handler in angular not reading op...   \n",
       "\n",
       "                                                     body  accepted_answer_id  \\\n",
       "166728  <p>I am trying to get the onPageSizeUpdate fun...            70703621   \n",
       "\n",
       "        view_count                                   tags  answer_count  \\\n",
       "166728          96  <angular><typescript><event-handling>             2   \n",
       "\n",
       "        question_score       creation_date  answer_score  \\\n",
       "166728               0 2022-01-13 21:25:46             0   \n",
       "\n",
       "                                     stackoverflow_answer  \\\n",
       "166728  <p>Instead of</p>&#xA;<pre><code>onPageSizeUpd...   \n",
       "\n",
       "                                           stripped_title  \\\n",
       "166728  change event handler in angular not reading op...   \n",
       "\n",
       "                                            stripped_body  \\\n",
       "166728  I am trying to get the onPageSizeUpdate functi...   \n",
       "\n",
       "                            stripped_stackoverflow_answer  \\\n",
       "166728  Instead of\\nonPageSizeUpdate($any($event.targe...   \n",
       "\n",
       "                                             gpt-4_answer  GPT_finished  \\\n",
       "166728  The issue is that you are not setting the valu...          True   \n",
       "\n",
       "                                        full_GPT_response  \n",
       "166728  {'id': 'chatcmpl-83JxCm9GK225VdHnBpjhwEUovO6dc...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check result of first chunk\n",
    "wd_list[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>accepted_answer_id</th>\n",
       "      <th>view_count</th>\n",
       "      <th>tags</th>\n",
       "      <th>answer_count</th>\n",
       "      <th>question_score</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>answer_score</th>\n",
       "      <th>stackoverflow_answer</th>\n",
       "      <th>stripped_title</th>\n",
       "      <th>stripped_body</th>\n",
       "      <th>stripped_stackoverflow_answer</th>\n",
       "      <th>gpt-4_answer</th>\n",
       "      <th>GPT_finished</th>\n",
       "      <th>full_GPT_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38630</th>\n",
       "      <td>69700210</td>\n",
       "      <td>How to display output in rows of five numbers?</td>\n",
       "      <td>&lt;p&gt;I'm new to programming and I have to displa...</td>\n",
       "      <td>69700396</td>\n",
       "      <td>315</td>\n",
       "      <td>&lt;c++&gt;&lt;loops&gt;&lt;vector&gt;&lt;counter&gt;&lt;primes&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-10-24 19:45:49</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;p&gt;You are seriously over-complicating your ou...</td>\n",
       "      <td>How to display output in rows of five numbers?</td>\n",
       "      <td>I'm new to programming and I have to display a...</td>\n",
       "      <td>You are seriously over-complicating your outpu...</td>\n",
       "      <td>Your code is almost correct. You are using the...</td>\n",
       "      <td>True</td>\n",
       "      <td>{'id': 'chatcmpl-83K22BAwXPoudciLqs7Fr5trhVbOF...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                           title  \\\n",
       "38630  69700210  How to display output in rows of five numbers?   \n",
       "\n",
       "                                                    body  accepted_answer_id  \\\n",
       "38630  <p>I'm new to programming and I have to displa...            69700396   \n",
       "\n",
       "       view_count                                   tags  answer_count  \\\n",
       "38630         315  <c++><loops><vector><counter><primes>             1   \n",
       "\n",
       "       question_score       creation_date  answer_score  \\\n",
       "38630               2 2021-10-24 19:45:49             3   \n",
       "\n",
       "                                    stackoverflow_answer  \\\n",
       "38630  <p>You are seriously over-complicating your ou...   \n",
       "\n",
       "                                       stripped_title  \\\n",
       "38630  How to display output in rows of five numbers?   \n",
       "\n",
       "                                           stripped_body  \\\n",
       "38630  I'm new to programming and I have to display a...   \n",
       "\n",
       "                           stripped_stackoverflow_answer  \\\n",
       "38630  You are seriously over-complicating your outpu...   \n",
       "\n",
       "                                            gpt-4_answer  GPT_finished  \\\n",
       "38630  Your code is almost correct. You are using the...          True   \n",
       "\n",
       "                                       full_GPT_response  \n",
       "38630  {'id': 'chatcmpl-83K22BAwXPoudciLqs7Fr5trhVbOF...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check result of last chun\n",
    "wd_list[9].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare JSONL file for evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip and save a copy of the GPT answer\n",
    "#   (so that the eval is on fair footing, with HTML tags removed from both human and AI)\n",
    "\n",
    "for index, chunk in enumerate(wd_list):\n",
    "\n",
    "    # Separate out the text columns we want for convenience\n",
    "    titles  = chunk[\"title\"]\n",
    "    bodies  = chunk[\"body\"]\n",
    "    answers = chunk[\"stackoverflow_answer\"]\n",
    "\n",
    "    # Create a new list to store HTML-stripped versions of text\n",
    "    s_gpt_answers  = []\n",
    "\n",
    "    # Iterating is slow(!) but comparatively easy to understand (modified Harvey approach)\n",
    "    for idx, *row in chunk.itertuples():\n",
    "        \n",
    "        # Strip HTML from GPT's answer; save to list\n",
    "        s_gpt_answers.append(soup(chunk[COL_NAME][idx], \"html.parser\").get_text())\n",
    "\n",
    "    # Add the populated lists into our dataframe\n",
    "    stripped_col_name = \"stripped_\" + COL_NAME\n",
    "    chunk[stripped_col_name] = s_gpt_answers\n",
    "\n",
    "    # Preview result\n",
    "    #chunk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>accepted_answer_id</th>\n",
       "      <th>view_count</th>\n",
       "      <th>tags</th>\n",
       "      <th>answer_count</th>\n",
       "      <th>question_score</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>answer_score</th>\n",
       "      <th>stackoverflow_answer</th>\n",
       "      <th>stripped_title</th>\n",
       "      <th>stripped_body</th>\n",
       "      <th>stripped_stackoverflow_answer</th>\n",
       "      <th>gpt-4_answer</th>\n",
       "      <th>GPT_finished</th>\n",
       "      <th>full_GPT_response</th>\n",
       "      <th>stripped_gpt-4_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>166728</th>\n",
       "      <td>70703290</td>\n",
       "      <td>change event handler in angular not reading op...</td>\n",
       "      <td>&lt;p&gt;I am trying to get the onPageSizeUpdate fun...</td>\n",
       "      <td>70703621</td>\n",
       "      <td>96</td>\n",
       "      <td>&lt;angular&gt;&lt;typescript&gt;&lt;event-handling&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-13 21:25:46</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;p&gt;Instead of&lt;/p&gt;&amp;#xA;&lt;pre&gt;&lt;code&gt;onPageSizeUpd...</td>\n",
       "      <td>change event handler in angular not reading op...</td>\n",
       "      <td>I am trying to get the onPageSizeUpdate functi...</td>\n",
       "      <td>Instead of\\nonPageSizeUpdate($any($event.targe...</td>\n",
       "      <td>The issue is that you are not setting the valu...</td>\n",
       "      <td>True</td>\n",
       "      <td>{'id': 'chatcmpl-83JxCm9GK225VdHnBpjhwEUovO6dc...</td>\n",
       "      <td>The issue is that you are not setting the valu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                              title  \\\n",
       "166728  70703290  change event handler in angular not reading op...   \n",
       "\n",
       "                                                     body  accepted_answer_id  \\\n",
       "166728  <p>I am trying to get the onPageSizeUpdate fun...            70703621   \n",
       "\n",
       "        view_count                                   tags  answer_count  \\\n",
       "166728          96  <angular><typescript><event-handling>             2   \n",
       "\n",
       "        question_score       creation_date  answer_score  \\\n",
       "166728               0 2022-01-13 21:25:46             0   \n",
       "\n",
       "                                     stackoverflow_answer  \\\n",
       "166728  <p>Instead of</p>&#xA;<pre><code>onPageSizeUpd...   \n",
       "\n",
       "                                           stripped_title  \\\n",
       "166728  change event handler in angular not reading op...   \n",
       "\n",
       "                                            stripped_body  \\\n",
       "166728  I am trying to get the onPageSizeUpdate functi...   \n",
       "\n",
       "                            stripped_stackoverflow_answer  \\\n",
       "166728  Instead of\\nonPageSizeUpdate($any($event.targe...   \n",
       "\n",
       "                                             gpt-4_answer  GPT_finished  \\\n",
       "166728  The issue is that you are not setting the valu...          True   \n",
       "\n",
       "                                        full_GPT_response  \\\n",
       "166728  {'id': 'chatcmpl-83JxCm9GK225VdHnBpjhwEUovO6dc...   \n",
       "\n",
       "                                    stripped_gpt-4_answer  \n",
       "166728  The issue is that you are not setting the valu...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check first chunk\n",
    "wd_list[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>accepted_answer_id</th>\n",
       "      <th>view_count</th>\n",
       "      <th>tags</th>\n",
       "      <th>answer_count</th>\n",
       "      <th>question_score</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>answer_score</th>\n",
       "      <th>stackoverflow_answer</th>\n",
       "      <th>stripped_title</th>\n",
       "      <th>stripped_body</th>\n",
       "      <th>stripped_stackoverflow_answer</th>\n",
       "      <th>gpt-4_answer</th>\n",
       "      <th>GPT_finished</th>\n",
       "      <th>full_GPT_response</th>\n",
       "      <th>stripped_gpt-4_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38630</th>\n",
       "      <td>69700210</td>\n",
       "      <td>How to display output in rows of five numbers?</td>\n",
       "      <td>&lt;p&gt;I'm new to programming and I have to displa...</td>\n",
       "      <td>69700396</td>\n",
       "      <td>315</td>\n",
       "      <td>&lt;c++&gt;&lt;loops&gt;&lt;vector&gt;&lt;counter&gt;&lt;primes&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-10-24 19:45:49</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;p&gt;You are seriously over-complicating your ou...</td>\n",
       "      <td>How to display output in rows of five numbers?</td>\n",
       "      <td>I'm new to programming and I have to display a...</td>\n",
       "      <td>You are seriously over-complicating your outpu...</td>\n",
       "      <td>Your code is almost correct. You are using the...</td>\n",
       "      <td>True</td>\n",
       "      <td>{'id': 'chatcmpl-83K22BAwXPoudciLqs7Fr5trhVbOF...</td>\n",
       "      <td>Your code is almost correct. You are using the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                           title  \\\n",
       "38630  69700210  How to display output in rows of five numbers?   \n",
       "\n",
       "                                                    body  accepted_answer_id  \\\n",
       "38630  <p>I'm new to programming and I have to displa...            69700396   \n",
       "\n",
       "       view_count                                   tags  answer_count  \\\n",
       "38630         315  <c++><loops><vector><counter><primes>             1   \n",
       "\n",
       "       question_score       creation_date  answer_score  \\\n",
       "38630               2 2021-10-24 19:45:49             3   \n",
       "\n",
       "                                    stackoverflow_answer  \\\n",
       "38630  <p>You are seriously over-complicating your ou...   \n",
       "\n",
       "                                       stripped_title  \\\n",
       "38630  How to display output in rows of five numbers?   \n",
       "\n",
       "                                           stripped_body  \\\n",
       "38630  I'm new to programming and I have to display a...   \n",
       "\n",
       "                           stripped_stackoverflow_answer  \\\n",
       "38630  You are seriously over-complicating your outpu...   \n",
       "\n",
       "                                            gpt-4_answer  GPT_finished  \\\n",
       "38630  Your code is almost correct. You are using the...          True   \n",
       "\n",
       "                                       full_GPT_response  \\\n",
       "38630  {'id': 'chatcmpl-83K22BAwXPoudciLqs7Fr5trhVbOF...   \n",
       "\n",
       "                                   stripped_gpt-4_answer  \n",
       "38630  Your code is almost correct. You are using the...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check last chunk\n",
    "wd_list[9].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSONL file: c:\\Users\\Mark\\Documents\\A2I2 T2 2023\\eval_samples\\new_samples_0.jsonl\n",
      "Saved JSONL file: c:\\Users\\Mark\\Documents\\A2I2 T2 2023\\eval_samples\\new_samples_1.jsonl\n",
      "Saved JSONL file: c:\\Users\\Mark\\Documents\\A2I2 T2 2023\\eval_samples\\new_samples_2.jsonl\n",
      "Saved JSONL file: c:\\Users\\Mark\\Documents\\A2I2 T2 2023\\eval_samples\\new_samples_3.jsonl\n",
      "Saved JSONL file: c:\\Users\\Mark\\Documents\\A2I2 T2 2023\\eval_samples\\new_samples_4.jsonl\n",
      "Saved JSONL file: c:\\Users\\Mark\\Documents\\A2I2 T2 2023\\eval_samples\\new_samples_5.jsonl\n",
      "Saved JSONL file: c:\\Users\\Mark\\Documents\\A2I2 T2 2023\\eval_samples\\new_samples_6.jsonl\n",
      "Saved JSONL file: c:\\Users\\Mark\\Documents\\A2I2 T2 2023\\eval_samples\\new_samples_7.jsonl\n",
      "Saved JSONL file: c:\\Users\\Mark\\Documents\\A2I2 T2 2023\\eval_samples\\new_samples_8.jsonl\n",
      "Saved JSONL file: c:\\Users\\Mark\\Documents\\A2I2 T2 2023\\eval_samples\\new_samples_9.jsonl\n"
     ]
    }
   ],
   "source": [
    "for index, chunk in enumerate(wd_list):\n",
    "\n",
    "    json_list = []\n",
    "    for idx, row in chunk.iterrows():\n",
    "        \n",
    "        # Skip if the GPT answer is either unfinished or not present (e.g. token limit)\n",
    "        #   Note: this has never been explicitly tested because none of my samples hit these conditions\n",
    "        if (row[col_name] == None) or (row[\"GPT_finished\"] == False):\n",
    "            continue\n",
    "\n",
    "        # Strip and chat-format the text as appropriate\n",
    "        title_and_question = row[\"stripped_title\"] + \"\\n\\n\" + row[\"stripped_body\"]\n",
    "        prompt = chat_format(title_and_question)\n",
    "\n",
    "        # Create the JSON object using the stripped and chat-formatted text\n",
    "        json_object = {\n",
    "            \"input\": prompt,\n",
    "            \"ideal\": row[\"stripped_stackoverflow_answer\"],\n",
    "            \"completion\": row[stripped_col_name]\n",
    "        }\n",
    "\n",
    "        # Add the object to our list\n",
    "        json_list.append(json.dumps(json_object))\n",
    "\n",
    "    # because re-running the notebook changes the sampling we DON'T want a persistent jsonl file\n",
    "    #   this step relies on the directories existing; they are not created here\n",
    "    #   so this will fail with e.g. FileNotFoundError if they don't exist\n",
    "    JSONL_FILENAME = f\"new_samples_{index}.jsonl\"\n",
    "    JSONL_FILEPATH = os.path.join(cwd, \"eval_samples\", JSONL_FILENAME)\n",
    "    with open(JSONL_FILEPATH, \"w\") as outfile:\n",
    "        outfile.write(\"\\n\".join(json_list))\n",
    "        print(\"Saved JSONL file: \" + JSONL_FILEPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install OpenAI evals\n",
    "\n",
    "As of writing, the version of evals on pip is wildly different from the version of evals available on GitHub (and even worse, they share the same version tag despite this). As such, even though we're not making our own eval we want the current version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'evals'...\n",
      "Updating files:  41% (490/1194)\n",
      "Updating files:  42% (502/1194)\n",
      "Updating files:  43% (514/1194)\n",
      "Updating files:  44% (526/1194)\n",
      "Updating files:  45% (538/1194)\n",
      "Updating files:  46% (550/1194)\n",
      "Updating files:  47% (562/1194)\n",
      "Updating files:  48% (574/1194)\n",
      "Updating files:  49% (586/1194)\n",
      "Updating files:  50% (597/1194)\n",
      "Updating files:  51% (609/1194)\n",
      "Updating files:  52% (621/1194)\n",
      "Updating files:  53% (633/1194)\n",
      "Updating files:  54% (645/1194)\n",
      "Updating files:  55% (657/1194)\n",
      "Updating files:  56% (669/1194)\n",
      "Updating files:  57% (681/1194)\n",
      "Updating files:  58% (693/1194)\n",
      "Updating files:  59% (705/1194)\n",
      "Updating files:  60% (717/1194)\n",
      "Updating files:  61% (729/1194)\n",
      "Updating files:  62% (741/1194)\n",
      "Updating files:  63% (753/1194)\n",
      "Updating files:  64% (765/1194)\n",
      "Updating files:  65% (777/1194)\n",
      "Updating files:  66% (789/1194)\n",
      "Updating files:  67% (800/1194)\n",
      "Updating files:  68% (812/1194)\n",
      "Updating files:  69% (824/1194)\n",
      "Updating files:  70% (836/1194)\n",
      "Updating files:  71% (848/1194)\n",
      "Updating files:  72% (860/1194)\n",
      "Updating files:  73% (872/1194)\n",
      "Updating files:  74% (884/1194)\n",
      "Updating files:  75% (896/1194)\n",
      "Updating files:  76% (908/1194)\n",
      "Updating files:  77% (920/1194)\n",
      "Updating files:  78% (932/1194)\n",
      "Updating files:  79% (944/1194)\n",
      "Updating files:  80% (956/1194)\n",
      "Updating files:  81% (968/1194)\n",
      "Updating files:  82% (980/1194)\n",
      "Updating files:  83% (992/1194)\n",
      "Updating files:  84% (1003/1194)\n",
      "Updating files:  85% (1015/1194)\n",
      "Updating files:  86% (1027/1194)\n",
      "Updating files:  87% (1039/1194)\n",
      "Updating files:  88% (1051/1194)\n",
      "Updating files:  89% (1063/1194)\n",
      "Updating files:  90% (1075/1194)\n",
      "Updating files:  91% (1087/1194)\n",
      "Updating files:  92% (1099/1194)\n",
      "Updating files:  93% (1111/1194)\n",
      "Updating files:  94% (1123/1194)\n",
      "Updating files:  95% (1135/1194)\n",
      "Updating files:  96% (1147/1194)\n",
      "Updating files:  97% (1159/1194)\n",
      "Updating files:  98% (1171/1194)\n",
      "Updating files:  99% (1183/1194)\n",
      "Updating files: 100% (1194/1194)\n",
      "Updating files: 100% (1194/1194), done.\n",
      "Filtering content:   2% (12/572), 13.69 MiB | 21.29 MiB/s\n",
      "Filtering content:   3% (18/572), 13.69 MiB | 21.29 MiB/s\n",
      "Filtering content:   4% (23/572), 24.26 MiB | 19.15 MiB/s\n",
      "Filtering content:   5% (29/572), 24.26 MiB | 19.15 MiB/s\n",
      "Filtering content:   6% (35/572), 24.26 MiB | 19.15 MiB/s\n",
      "Filtering content:   6% (38/572), 24.26 MiB | 19.15 MiB/s\n",
      "Filtering content:   7% (41/572), 24.26 MiB | 19.15 MiB/s\n",
      "Filtering content:   8% (46/572), 25.94 MiB | 14.35 MiB/s\n",
      "Filtering content:   9% (52/572), 25.94 MiB | 14.35 MiB/s\n",
      "Filtering content:  10% (58/572), 26.41 MiB | 11.26 MiB/s\n",
      "Filtering content:  11% (63/572), 26.41 MiB | 11.26 MiB/s\n",
      "Filtering content:  11% (64/572), 26.41 MiB | 11.26 MiB/s\n",
      "Filtering content:  12% (69/572), 26.70 MiB | 9.16 MiB/s \n",
      "Filtering content:  13% (75/572), 26.70 MiB | 9.16 MiB/s\n",
      "Filtering content:  14% (81/572), 26.70 MiB | 9.16 MiB/s\n",
      "Filtering content:  15% (86/572), 26.87 MiB | 7.81 MiB/s\n",
      "Filtering content:  15% (91/572), 26.87 MiB | 7.81 MiB/s\n",
      "Filtering content:  16% (92/572), 26.87 MiB | 7.81 MiB/s\n",
      "Filtering content:  17% (98/572), 26.95 MiB | 6.74 MiB/s\n",
      "Filtering content:  17% (101/572), 26.95 MiB | 6.74 MiB/s\n",
      "Filtering content:  18% (103/572), 28.06 MiB | 5.10 MiB/s\n",
      "Filtering content:  18% (105/572), 28.06 MiB | 5.10 MiB/s\n",
      "Filtering content:  19% (109/572), 36.25 MiB | 5.89 MiB/s\n",
      "Filtering content:  19% (110/572), 48.27 MiB | 5.90 MiB/s\n",
      "Filtering content:  20% (115/572), 48.27 MiB | 5.90 MiB/s\n",
      "Filtering content:  21% (121/572), 51.45 MiB | 4.65 MiB/s\n",
      "Filtering content:  22% (126/572), 53.44 MiB | 4.64 MiB/s\n",
      "Filtering content:  22% (131/572), 54.52 MiB | 4.68 MiB/s\n",
      "Filtering content:  23% (132/572), 54.92 MiB | 4.71 MiB/s\n",
      "Filtering content:  24% (138/572), 55.53 MiB | 4.78 MiB/s\n",
      "Filtering content:  24% (142/572), 55.53 MiB | 4.78 MiB/s\n",
      "Filtering content:  25% (143/572), 55.53 MiB | 4.78 MiB/s\n",
      "Filtering content:  26% (149/572), 55.97 MiB | 4.77 MiB/s\n",
      "Filtering content:  26% (153/572), 56.24 MiB | 5.41 MiB/s\n",
      "Filtering content:  27% (155/572), 56.24 MiB | 5.41 MiB/s\n",
      "Filtering content:  28% (161/572), 137.08 MiB | 19.34 MiB/s\n",
      "Filtering content:  29% (166/572), 137.08 MiB | 19.34 MiB/s\n",
      "Filtering content:  30% (172/572), 137.08 MiB | 19.34 MiB/s\n",
      "Filtering content:  31% (178/572), 137.43 MiB | 17.08 MiB/s\n",
      "Filtering content:  32% (184/572), 137.55 MiB | 16.69 MiB/s\n",
      "Filtering content:  33% (189/572), 137.55 MiB | 16.69 MiB/s\n",
      "Filtering content:  33% (191/572), 137.55 MiB | 16.69 MiB/s\n",
      "Filtering content:  34% (195/572), 137.63 MiB | 16.61 MiB/s\n",
      "Filtering content:  35% (201/572), 300.39 MiB | 46.00 MiB/s\n",
      "Filtering content:  35% (203/572), 311.17 MiB | 43.68 MiB/s\n",
      "Filtering content:  36% (206/572), 311.17 MiB | 43.68 MiB/s\n",
      "Filtering content:  37% (212/572), 332.23 MiB | 45.87 MiB/s\n",
      "Filtering content:  38% (218/572), 348.00 MiB | 49.40 MiB/s\n",
      "Filtering content:  39% (224/572), 348.00 MiB | 49.40 MiB/s\n",
      "Filtering content:  40% (229/572), 359.35 MiB | 51.04 MiB/s\n",
      "Filtering content:  41% (235/572), 359.35 MiB | 51.04 MiB/s\n",
      "Filtering content:  42% (241/572), 359.35 MiB | 51.04 MiB/s\n",
      "Filtering content:  43% (246/572), 360.45 MiB | 38.69 MiB/s\n",
      "Filtering content:  44% (252/572), 360.45 MiB | 38.69 MiB/s\n",
      "Filtering content:  45% (258/572), 360.93 MiB | 38.86 MiB/s\n",
      "Filtering content:  45% (263/572), 360.93 MiB | 38.86 MiB/s\n",
      "Filtering content:  46% (264/572), 360.93 MiB | 38.86 MiB/s\n",
      "Filtering content:  47% (269/572), 360.93 MiB | 38.86 MiB/s\n",
      "Filtering content:  48% (275/572), 361.20 MiB | 38.80 MiB/s\n",
      "Filtering content:  49% (281/572), 361.20 MiB | 38.80 MiB/s\n",
      "Filtering content:  50% (286/572), 361.20 MiB | 38.80 MiB/s\n",
      "Filtering content:  50% (291/572), 361.34 MiB | 38.69 MiB/s\n",
      "Filtering content:  51% (292/572), 361.34 MiB | 38.69 MiB/s\n",
      "Filtering content:  52% (298/572), 361.34 MiB | 38.69 MiB/s\n",
      "Filtering content:  52% (301/572), 361.34 MiB | 38.69 MiB/s\n",
      "Filtering content:  52% (302/572), 366.39 MiB | 9.33 MiB/s \n",
      "Filtering content:  53% (304/572), 366.39 MiB | 9.33 MiB/s\n",
      "Filtering content:  54% (309/572), 388.64 MiB | 11.78 MiB/s\n",
      "Filtering content:  54% (310/572), 388.64 MiB | 11.78 MiB/s\n",
      "Filtering content:  55% (315/572), 397.75 MiB | 10.20 MiB/s\n",
      "Filtering content:  55% (319/572), 413.74 MiB | 9.91 MiB/s \n",
      "Filtering content:  56% (321/572), 413.74 MiB | 9.91 MiB/s\n",
      "Filtering content:  57% (327/572), 419.98 MiB | 9.18 MiB/s\n",
      "Filtering content:  58% (332/572), 419.98 MiB | 9.18 MiB/s\n",
      "Filtering content:  59% (338/572), 426.41 MiB | 9.87 MiB/s\n",
      "Filtering content:  60% (344/572), 426.41 MiB | 9.87 MiB/s\n",
      "Filtering content:  61% (349/572), 427.50 MiB | 9.91 MiB/s\n",
      "Filtering content:  62% (355/572), 427.50 MiB | 9.91 MiB/s\n",
      "Filtering content:  63% (361/572), 427.50 MiB | 9.91 MiB/s\n",
      "Filtering content:  63% (365/572), 428.32 MiB | 9.98 MiB/s\n",
      "Filtering content:  64% (367/572), 428.32 MiB | 9.98 MiB/s\n",
      "Filtering content:  65% (372/572), 428.32 MiB | 9.98 MiB/s\n",
      "Filtering content:  66% (378/572), 428.32 MiB | 9.98 MiB/s\n",
      "Filtering content:  67% (384/572), 428.61 MiB | 10.00 MiB/s\n",
      "Filtering content:  68% (389/572), 428.61 MiB | 10.00 MiB/s\n",
      "Filtering content:  69% (395/572), 428.61 MiB | 10.00 MiB/s\n",
      "Filtering content:  69% (396/572), 428.61 MiB | 10.00 MiB/s\n",
      "Filtering content:  70% (401/572), 428.73 MiB | 12.29 MiB/s\n",
      "Filtering content:  70% (403/572), 440.86 MiB | 7.24 MiB/s \n",
      "Filtering content:  71% (407/572), 463.92 MiB | 8.97 MiB/s\n",
      "Filtering content:  71% (410/572), 517.50 MiB | 14.18 MiB/s\n",
      "Filtering content:  72% (412/572), 517.50 MiB | 14.18 MiB/s\n",
      "Filtering content:  72% (415/572), 538.91 MiB | 16.02 MiB/s\n",
      "Filtering content:  73% (418/572), 538.91 MiB | 16.02 MiB/s\n",
      "Filtering content:  74% (424/572), 562.87 MiB | 18.18 MiB/s\n",
      "Filtering content:  75% (429/572), 562.87 MiB | 18.18 MiB/s\n",
      "Filtering content:  76% (435/572), 576.48 MiB | 19.93 MiB/s\n",
      "Filtering content:  76% (436/572), 576.48 MiB | 19.93 MiB/s\n",
      "Filtering content:  77% (441/572), 576.48 MiB | 19.93 MiB/s\n",
      "Filtering content:  78% (447/572), 576.48 MiB | 19.93 MiB/s\n",
      "Filtering content:  79% (452/572), 577.42 MiB | 20.10 MiB/s\n",
      "Filtering content:  80% (458/572), 577.42 MiB | 20.10 MiB/s\n",
      "Filtering content:  81% (464/572), 577.81 MiB | 20.16 MiB/s\n",
      "Filtering content:  81% (465/572), 577.81 MiB | 20.16 MiB/s\n",
      "Filtering content:  82% (470/572), 577.81 MiB | 20.16 MiB/s\n",
      "Filtering content:  83% (475/572), 578.00 MiB | 20.17 MiB/s\n",
      "Filtering content:  84% (481/572), 578.00 MiB | 20.17 MiB/s\n",
      "Filtering content:  85% (487/572), 578.00 MiB | 20.17 MiB/s\n",
      "Filtering content:  86% (492/572), 578.11 MiB | 26.30 MiB/s\n",
      "Filtering content:  86% (493/572), 578.11 MiB | 26.30 MiB/s\n",
      "Filtering content:  87% (498/572), 578.11 MiB | 26.30 MiB/s\n",
      "Filtering content:  87% (501/572), 578.11 MiB | 26.30 MiB/s\n",
      "Filtering content:  88% (504/572), 579.27 MiB | 18.39 MiB/s\n",
      "Filtering content:  88% (506/572), 579.27 MiB | 18.39 MiB/s\n",
      "Filtering content:  89% (510/572), 603.97 MiB | 14.00 MiB/s\n",
      "Filtering content:  90% (515/572), 603.97 MiB | 14.00 MiB/s\n",
      "Filtering content:  91% (521/572), 603.97 MiB | 14.00 MiB/s\n",
      "Filtering content:  92% (527/572), 607.81 MiB | 11.42 MiB/s\n",
      "Filtering content:  93% (532/572), 607.81 MiB | 11.42 MiB/s\n",
      "Filtering content:  93% (534/572), 607.81 MiB | 11.42 MiB/s\n",
      "Filtering content:  94% (538/572), 607.81 MiB | 11.42 MiB/s\n",
      "Filtering content:  95% (544/572), 608.96 MiB | 7.77 MiB/s \n",
      "Filtering content:  96% (550/572), 608.96 MiB | 7.77 MiB/s\n",
      "Filtering content:  97% (555/572), 609.31 MiB | 5.52 MiB/s\n",
      "Filtering content:  97% (560/572), 609.31 MiB | 5.52 MiB/s\n",
      "Filtering content:  98% (561/572), 609.31 MiB | 5.52 MiB/s\n",
      "Filtering content:  99% (567/572), 609.31 MiB | 5.52 MiB/s\n",
      "Filtering content: 100% (572/572), 609.41 MiB | 5.36 MiB/s\n",
      "Filtering content: 100% (572/572), 609.41 MiB | 15.78 MiB/s, done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetch: Fetching all references...\n",
      "Obtaining file:///C:/Users/Mark/Documents/A2I2%20T2%202023/evals\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Checking if build backend supports build_editable: started\n",
      "  Checking if build backend supports build_editable: finished with status 'done'\n",
      "  Getting requirements to build editable: started\n",
      "  Getting requirements to build editable: finished with status 'done'\n",
      "  Preparing editable metadata (pyproject.toml): started\n",
      "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: mypy in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (1.5.1)\n",
      "Requirement already satisfied: openai>=0.27.2 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (0.28.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (0.4.0)\n",
      "Requirement already satisfied: blobfile in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (2.0.2)\n",
      "Requirement already satisfied: backoff in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (2.2.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (1.25.2)\n",
      "Requirement already satisfied: snowflake-connector-python[pandas] in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (3.1.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (2.0.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (2.14.5)\n",
      "Requirement already satisfied: fire in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (0.5.0)\n",
      "Requirement already satisfied: pydantic in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (2.3.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (4.66.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (3.8.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (3.12.3)\n",
      "Requirement already satisfied: mock in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (5.1.0)\n",
      "Requirement already satisfied: langdetect in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (1.0.9)\n",
      "Requirement already satisfied: termcolor in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (2.3.0)\n",
      "Requirement already satisfied: lz4 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (4.3.2)\n",
      "Requirement already satisfied: pyzstd in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (0.15.9)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (6.0.1)\n",
      "Requirement already satisfied: sacrebleu in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (2.3.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (3.7.2)\n",
      "Requirement already satisfied: pytest in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (7.4.1)\n",
      "Requirement already satisfied: langchain in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (0.0.283)\n",
      "Requirement already satisfied: types-PyYAML in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (6.0.12.11)\n",
      "Collecting spacy-universal-sentence-encoder (from evals==1.0.3.post1)\n",
      "  Downloading spacy_universal_sentence_encoder-0.4.6.tar.gz (15 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting jiwer (from evals==1.0.3.post1)\n",
      "  Obtaining dependency information for jiwer from https://files.pythonhosted.org/packages/0d/4f/ee537ab20144811dd99321735ff92ef2b3a3230b77ed7454bed4c44d21fc/jiwer-3.0.3-py3-none-any.whl.metadata\n",
      "  Downloading jiwer-3.0.3-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from openai>=0.27.2->evals==1.0.3.post1) (2.31.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from openai>=0.27.2->evals==1.0.3.post1) (3.8.5)\n",
      "Requirement already satisfied: pycryptodomex~=3.8 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from blobfile->evals==1.0.3.post1) (3.18.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from blobfile->evals==1.0.3.post1) (1.26.16)\n",
      "Requirement already satisfied: lxml~=4.9 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from blobfile->evals==1.0.3.post1) (4.9.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from datasets->evals==1.0.3.post1) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from datasets->evals==1.0.3.post1) (0.3.7)\n",
      "Requirement already satisfied: xxhash in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from datasets->evals==1.0.3.post1) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from datasets->evals==1.0.3.post1) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from datasets->evals==1.0.3.post1) (2023.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from datasets->evals==1.0.3.post1) (0.16.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from datasets->evals==1.0.3.post1) (23.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from tqdm->evals==1.0.3.post1) (0.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from fire->evals==1.0.3.post1) (1.16.0)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from jiwer->evals==1.0.3.post1) (8.1.7)\n",
      "Collecting rapidfuzz<4,>=3 (from jiwer->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for rapidfuzz<4,>=3 from https://files.pythonhosted.org/packages/ba/74/f7656506c813e95ea8cc8698f1ab17e2ddf0db6a73d0ee0cbaf608388853/rapidfuzz-3.3.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading rapidfuzz-3.3.1-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from langchain->evals==1.0.3.post1) (2.0.20)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from langchain->evals==1.0.3.post1) (0.5.14)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.21 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from langchain->evals==1.0.3.post1) (0.0.33)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from langchain->evals==1.0.3.post1) (2.8.5)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from langchain->evals==1.0.3.post1) (8.2.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from pydantic->evals==1.0.3.post1) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.6.3 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from pydantic->evals==1.0.3.post1) (2.6.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from pydantic->evals==1.0.3.post1) (4.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from matplotlib->evals==1.0.3.post1) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from matplotlib->evals==1.0.3.post1) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from matplotlib->evals==1.0.3.post1) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from matplotlib->evals==1.0.3.post1) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from matplotlib->evals==1.0.3.post1) (10.0.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from matplotlib->evals==1.0.3.post1) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from matplotlib->evals==1.0.3.post1) (2.8.2)\n",
      "Requirement already satisfied: mypy-extensions>=1.0.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from mypy->evals==1.0.3.post1) (1.0.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from nltk->evals==1.0.3.post1) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from nltk->evals==1.0.3.post1) (2023.8.8)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from pandas->evals==1.0.3.post1) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from pandas->evals==1.0.3.post1) (2023.3)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from pytest->evals==1.0.3.post1) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from pytest->evals==1.0.3.post1) (1.3.0)\n",
      "Requirement already satisfied: portalocker in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from sacrebleu->evals==1.0.3.post1) (2.7.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from sacrebleu->evals==1.0.3.post1) (0.9.0)\n",
      "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from snowflake-connector-python[pandas]->evals==1.0.3.post1) (1.5.1)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from snowflake-connector-python[pandas]->evals==1.0.3.post1) (1.15.1)\n",
      "Requirement already satisfied: cryptography<42.0.0,>=3.1.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from snowflake-connector-python[pandas]->evals==1.0.3.post1) (41.0.3)\n",
      "Requirement already satisfied: oscrypto<2.0.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from snowflake-connector-python[pandas]->evals==1.0.3.post1) (1.3.0)\n",
      "Requirement already satisfied: pyOpenSSL<24.0.0,>=16.2.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from snowflake-connector-python[pandas]->evals==1.0.3.post1) (23.2.0)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from snowflake-connector-python[pandas]->evals==1.0.3.post1) (2.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from snowflake-connector-python[pandas]->evals==1.0.3.post1) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from snowflake-connector-python[pandas]->evals==1.0.3.post1) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from snowflake-connector-python[pandas]->evals==1.0.3.post1) (2023.7.22)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from snowflake-connector-python[pandas]->evals==1.0.3.post1) (2.4.0)\n",
      "Collecting platformdirs<3.9.0,>=2.6.0 (from snowflake-connector-python[pandas]->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for platformdirs<3.9.0,>=2.6.0 from https://files.pythonhosted.org/packages/9e/d8/563a9fc17153c588c8c2042d2f0f84a89057cdb1c30270f589c88b42d62c/platformdirs-3.8.1-py3-none-any.whl.metadata\n",
      "  Using cached platformdirs-3.8.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: tomlkit in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from snowflake-connector-python[pandas]->evals==1.0.3.post1) (0.12.1)\n",
      "Collecting pyarrow>=8.0.0 (from datasets->evals==1.0.3.post1)\n",
      "  Using cached pyarrow-10.0.1-cp311-cp311-win_amd64.whl (20.2 MB)\n",
      "Collecting tensorflow<3.0.0,>=2.4.0 (from spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for tensorflow<3.0.0,>=2.4.0 from https://files.pythonhosted.org/packages/80/6f/57d36f6507e432d7fc1956b2e9e8530c5c2d2bfcd8821bcbfae271cd6688/tensorflow-2.14.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading tensorflow-2.14.0-cp311-cp311-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting spacy<4.0.0,>=3.0.0 (from spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for spacy<4.0.0,>=3.0.0 from https://files.pythonhosted.org/packages/d6/9e/8afc618cfed4b5dc602b11754d4d9193a268439704defae301bffca7f04c/spacy-3.6.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading spacy-3.6.1-cp311-cp311-win_amd64.whl.metadata (26 kB)\n",
      "Collecting tensorflow-hub (from spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for tensorflow-hub from https://files.pythonhosted.org/packages/30/78/9d5292a2b616901bdb075bbf0c777b293f4140bb48108ac2b33fd716c2eb/tensorflow_hub-0.14.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorflow_hub-0.14.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from aiohttp->openai>=0.27.2->evals==1.0.3.post1) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from aiohttp->openai>=0.27.2->evals==1.0.3.post1) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from aiohttp->openai>=0.27.2->evals==1.0.3.post1) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from aiohttp->openai>=0.27.2->evals==1.0.3.post1) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from aiohttp->openai>=0.27.2->evals==1.0.3.post1) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from aiohttp->openai>=0.27.2->evals==1.0.3.post1) (1.3.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python[pandas]->evals==1.0.3.post1) (2.21)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->evals==1.0.3.post1) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->evals==1.0.3.post1) (0.9.0)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for spacy-loggers<2.0.0,>=1.0.0 from https://files.pythonhosted.org/packages/33/78/d1a1a026ef3af911159398c939b1509d5c36fe524c7b644f34a5146c4e16/spacy_loggers-1.0.5-py3-none-any.whl.metadata\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for murmurhash<1.1.0,>=0.28.0 from https://files.pythonhosted.org/packages/71/46/af01a20ec368bd9cb49a1d2df15e3eca113bbf6952cc1f2a47f1c6801a7f/murmurhash-1.0.10-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading murmurhash-1.0.10-cp311-cp311-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for cymem<2.1.0,>=2.0.2 from https://files.pythonhosted.org/packages/c1/c3/dd044e6f62a3d317c461f6f0c153c6573ed13025752d779e514000c15dd2/cymem-2.0.8-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading cymem-2.0.8-cp311-cp311-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for preshed<3.1.0,>=3.0.2 from https://files.pythonhosted.org/packages/e4/fc/78cdbdb79f5d6d45949e72c32445d6c060977ad50a1dcfc0392622165f7c/preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.2.0,>=8.1.8 (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for thinc<8.2.0,>=8.1.8 from https://files.pythonhosted.org/packages/ea/65/9fe6fe1ddb5fd34b7b81dada121e6862791e624384a2964331d0228aea38/thinc-8.1.12-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading thinc-8.1.12-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for wasabi<1.2.0,>=0.9.1 from https://files.pythonhosted.org/packages/8f/69/26cbf0bad11703241cb84d5324d868097f7a8faf2f1888354dac8883f3fc/wasabi-1.1.2-py3-none-any.whl.metadata\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for srsly<3.0.0,>=2.4.3 from https://files.pythonhosted.org/packages/eb/f5/e3f29993f673d91623df6413ba64e815dd2676fd7932cbc5e7347402ddae/srsly-2.4.8-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading srsly-2.4.8-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for catalogue<2.1.0,>=2.0.6 from https://files.pythonhosted.org/packages/9e/96/d32b941a501ab566a16358d68b6eb4e4acc373fab3c3c4d7d9e649f7b4bb/catalogue-2.0.10-py3-none-any.whl.metadata\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 0.0/45.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 45.9/45.9 kB 2.2 MB/s eta 0:00:00\n",
      "Collecting pathy>=0.10.0 (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for pathy>=0.10.0 from https://files.pythonhosted.org/packages/b5/c3/04a002ace658133f5ac48d30258ed9ceab720595dc1ac36df02fe52018af/pathy-0.10.2-py3-none-any.whl.metadata\n",
      "  Downloading pathy-0.10.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1 (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for smart-open<7.0.0,>=5.2.1 from https://files.pythonhosted.org/packages/fc/d9/d97f1db64b09278aba64e8c81b5d322d436132df5741c518f3823824fae0/smart_open-6.4.0-py3-none-any.whl.metadata\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (68.0.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "     ---------------------------------------- 0.0/181.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 181.6/181.6 kB 3.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain->evals==1.0.3.post1) (2.0.2)\n",
      "Collecting tensorflow-intel==2.14.0 (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for tensorflow-intel==2.14.0 from https://files.pythonhosted.org/packages/ad/6e/1bfe367855dd87467564f7bf9fa14f3b17889988e79598bc37bf18f5ffb6/tensorflow_intel-2.14.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading tensorflow_intel-2.14.0-cp311-cp311-win_amd64.whl.metadata (4.8 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for absl-py>=1.0.0 from https://files.pythonhosted.org/packages/01/e4/dc0a1dcc4e74e08d7abedab278c795eef54a224363bb18f5692f416d834f/absl_py-2.0.0-py3-none-any.whl.metadata\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for flatbuffers>=23.5.26 from https://files.pythonhosted.org/packages/6f/12/d5c79ee252793ffe845d58a913197bfa02ae9a0b5c9bc3dc4b58d477b9e7/flatbuffers-23.5.26-py2.py3-none-any.whl.metadata\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 0.0/57.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.5/57.5 kB 3.1 MB/s eta 0:00:00\n",
      "Collecting h5py>=2.9.0 (from tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for h5py>=2.9.0 from https://files.pythonhosted.org/packages/d1/93/0f4cf5058095d749d464e4f770d2bf339930e5f3374331f0d2fa6ddfbf28/h5py-3.9.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading h5py-3.9.0-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/02/8c/dc970bc00867fe290e8c8a7befa1635af716a9ebdfe3fb9dce0ca4b522ce/libclang-16.0.6-py2.py3-none-win_amd64.whl.metadata\n",
      "  Downloading libclang-16.0.6-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting ml-dtypes==0.2.0 (from tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for ml-dtypes==0.2.0 from https://files.pythonhosted.org/packages/08/89/c727fde1a3d12586e0b8c01abf53754707d76beaa9987640e70807d4545f/ml_dtypes-0.2.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading ml_dtypes-0.2.0-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 0.0/65.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 65.5/65.5 kB 3.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (4.24.2)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Downloading wrapt-1.14.1.tar.gz (50 kB)\n",
      "     ---------------------------------------- 0.0/50.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 50.9/50.9 kB 2.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     ------- -------------------------------- 0.3/1.5 MB 8.9 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 0.7/1.5 MB 9.2 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 1.2/1.5 MB 9.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 9.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (1.57.0)\n",
      "Collecting tensorboard<2.15,>=2.14 (from tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for tensorboard<2.15,>=2.14 from https://files.pythonhosted.org/packages/bc/a2/ff5f4c299eb37c95299a76015da3f30211468e29d8d6f1d011683279baee/tensorboard-2.14.0-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard-2.14.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for tensorflow-estimator<2.15,>=2.14.0 from https://files.pythonhosted.org/packages/d1/da/4f264c196325bb6e37a6285caec5b12a03def489b57cc1fdac02bb6272cd/tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.15,>=2.14.0 (from tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for keras<2.15,>=2.14.0 from https://files.pythonhosted.org/packages/fe/58/34d4d8f1aa11120c2d36d7ad27d0526164b1a8ae45990a2fede31d0e59bf/keras-2.14.0-py3-none-any.whl.metadata\n",
      "  Downloading keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from portalocker->sacrebleu->evals==1.0.3.post1) (305.1)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for blis<0.8.0,>=0.7.8 from https://files.pythonhosted.org/packages/2f/09/da0592c74560cc33396504698122f7a56747c82a5e072ca7d2c3397898e1/blis-0.7.11-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading blis-0.7.11-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for confection<1.0.0,>=0.0.1 from https://files.pythonhosted.org/packages/93/f8/e89268a1f885048fb2ee6b5c9f93c4e90de768534acfef3652f87d97d4cb/confection-0.1.3-py3-none-any.whl.metadata\n",
      "  Downloading confection-0.1.3-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from jinja2->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (2.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (2.22.0)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/1a/b5/228c1cdcfe138f1a8e01ab1b54284c8b83735476cb22b6ba251656ed13ad/Markdown-3.4.4-py3-none-any.whl.metadata\n",
      "  Downloading Markdown-3.4.4-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/da/61/6e9ff8258422d287eec718872fb71e05324356722ab658c8afda25f51539/tensorboard_data_server-0.7.1-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for werkzeug>=1.0.1 from https://files.pythonhosted.org/packages/9b/59/a7c32e3d8d0e546a206e0552a2c04444544f15c1da4a01df8938d20c6ffc/werkzeug-2.3.7-py3-none-any.whl.metadata\n",
      "  Downloading werkzeug-2.3.7-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (4.9)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (0.5.0)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     ---------------------------------------- 0.0/151.7 kB ? eta -:--:--\n",
      "     ----------------------------------- -- 143.4/151.7 kB 4.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 151.7/151.7 kB 4.6 MB/s eta 0:00:00\n",
      "Downloading jiwer-3.0.3-py3-none-any.whl (21 kB)\n",
      "Using cached platformdirs-3.8.1-py3-none-any.whl (16 kB)\n",
      "Downloading rapidfuzz-3.3.1-cp311-cp311-win_amd64.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.2/1.8 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.6/1.8 MB 6.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.4/1.8 MB 9.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 10.6 MB/s eta 0:00:00\n",
      "Downloading spacy-3.6.1-cp311-cp311-win_amd64.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.9/12.0 MB 19.6 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 2.0/12.0 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 3.3/12.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 4.7/12.0 MB 25.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 6.3/12.0 MB 26.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 8.0/12.0 MB 28.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.7/12.0 MB 29.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.3/12.0 MB 31.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.0/12.0 MB 32.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 28.5 MB/s eta 0:00:00\n",
      "Downloading tensorflow-2.14.0-cp311-cp311-win_amd64.whl (2.1 kB)\n",
      "Downloading tensorflow_intel-2.14.0-cp311-cp311-win_amd64.whl (284.2 MB)\n",
      "   ---------------------------------------- 0.0/284.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/284.2 MB 31.0 MB/s eta 0:00:10\n",
      "   ---------------------------------------- 2.5/284.2 MB 31.9 MB/s eta 0:00:09\n",
      "    --------------------------------------- 4.2/284.2 MB 33.4 MB/s eta 0:00:09\n",
      "    --------------------------------------- 5.7/284.2 MB 33.0 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 7.1/284.2 MB 32.5 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 9.0/284.2 MB 33.9 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 10.9/284.2 MB 32.7 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 12.5/284.2 MB 36.3 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 14.2/284.2 MB 36.3 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 15.2/284.2 MB 36.4 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 16.3/284.2 MB 32.8 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 18.0/284.2 MB 32.8 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 19.7/284.2 MB 32.8 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 21.3/284.2 MB 32.7 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 23.0/284.2 MB 32.7 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 24.7/284.2 MB 32.7 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 26.4/284.2 MB 36.3 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 28.2/284.2 MB 36.4 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 29.8/284.2 MB 36.4 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 31.4/284.2 MB 36.4 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 32.8/284.2 MB 36.4 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 34.3/284.2 MB 34.4 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 36.0/284.2 MB 34.4 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 37.6/284.2 MB 34.4 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 39.2/284.2 MB 34.4 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 40.9/284.2 MB 36.4 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 42.6/284.2 MB 34.4 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 44.3/284.2 MB 36.4 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 45.8/284.2 MB 34.4 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 47.6/284.2 MB 36.4 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 49.1/284.2 MB 36.4 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 50.9/284.2 MB 36.4 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 52.4/284.2 MB 36.4 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 54.3/284.2 MB 36.4 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 56.0/284.2 MB 38.5 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 57.7/284.2 MB 36.3 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 59.2/284.2 MB 36.4 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 60.5/284.2 MB 34.4 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 60.8/284.2 MB 32.7 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 60.9/284.2 MB 27.3 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 60.9/284.2 MB 24.2 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 61.0/284.2 MB 22.6 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 61.6/284.2 MB 20.5 MB/s eta 0:00:11\n",
      "   -------- ------------------------------- 63.1/284.2 MB 21.1 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 64.7/284.2 MB 20.5 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 66.3/284.2 MB 21.1 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 68.0/284.2 MB 20.5 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 69.6/284.2 MB 20.5 MB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 71.2/284.2 MB 32.7 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 72.8/284.2 MB 34.4 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 74.4/284.2 MB 34.4 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 76.2/284.2 MB 36.3 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 77.7/284.2 MB 34.6 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 79.5/284.2 MB 36.4 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 81.2/284.2 MB 36.4 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 82.8/284.2 MB 36.4 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 84.4/284.2 MB 36.4 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 86.2/284.2 MB 36.4 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 87.9/284.2 MB 36.3 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 89.7/284.2 MB 36.3 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 91.4/284.2 MB 36.4 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 93.1/284.2 MB 36.4 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 94.7/284.2 MB 36.4 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 96.4/284.2 MB 36.4 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 98.2/284.2 MB 36.4 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 99.6/284.2 MB 38.6 MB/s eta 0:00:05\n",
      "   ------------- ------------------------- 101.8/284.2 MB 38.5 MB/s eta 0:00:05\n",
      "   -------------- ------------------------ 103.4/284.2 MB 36.3 MB/s eta 0:00:05\n",
      "   -------------- ------------------------ 105.1/284.2 MB 36.4 MB/s eta 0:00:05\n",
      "   -------------- ------------------------ 106.7/284.2 MB 36.4 MB/s eta 0:00:05\n",
      "   -------------- ------------------------ 108.3/284.2 MB 36.4 MB/s eta 0:00:05\n",
      "   --------------- ----------------------- 109.9/284.2 MB 36.4 MB/s eta 0:00:05\n",
      "   --------------- ----------------------- 111.6/284.2 MB 34.4 MB/s eta 0:00:06\n",
      "   --------------- ----------------------- 113.4/284.2 MB 36.4 MB/s eta 0:00:05\n",
      "   --------------- ----------------------- 115.1/284.2 MB 34.4 MB/s eta 0:00:05\n",
      "   ---------------- ---------------------- 116.9/284.2 MB 36.3 MB/s eta 0:00:05\n",
      "   ---------------- ---------------------- 118.4/284.2 MB 36.4 MB/s eta 0:00:05\n",
      "   ---------------- ---------------------- 119.9/284.2 MB 34.4 MB/s eta 0:00:05\n",
      "   ---------------- ---------------------- 121.7/284.2 MB 36.4 MB/s eta 0:00:05\n",
      "   ---------------- ---------------------- 123.4/284.2 MB 36.4 MB/s eta 0:00:05\n",
      "   ----------------- --------------------- 124.7/284.2 MB 36.4 MB/s eta 0:00:05\n",
      "   ----------------- --------------------- 126.8/284.2 MB 34.4 MB/s eta 0:00:05\n",
      "   ----------------- --------------------- 128.2/284.2 MB 36.4 MB/s eta 0:00:05\n",
      "   ----------------- --------------------- 130.0/284.2 MB 34.4 MB/s eta 0:00:05\n",
      "   ------------------ -------------------- 131.8/284.2 MB 38.5 MB/s eta 0:00:04\n",
      "   ------------------ -------------------- 133.4/284.2 MB 36.4 MB/s eta 0:00:05\n",
      "   ------------------ -------------------- 135.1/284.2 MB 38.5 MB/s eta 0:00:04\n",
      "   ------------------ -------------------- 136.5/284.2 MB 36.4 MB/s eta 0:00:05\n",
      "   ------------------ -------------------- 137.8/284.2 MB 32.7 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 138.9/284.2 MB 34.4 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 140.8/284.2 MB 34.4 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 142.7/284.2 MB 34.4 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 144.2/284.2 MB 34.4 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 146.0/284.2 MB 34.4 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 147.7/284.2 MB 36.3 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 149.2/284.2 MB 38.6 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 151.0/284.2 MB 36.4 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 152.6/284.2 MB 36.4 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 154.2/284.2 MB 36.4 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 156.1/284.2 MB 36.4 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 157.8/284.2 MB 36.4 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 159.4/284.2 MB 36.3 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 161.1/284.2 MB 36.3 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 163.0/284.2 MB 38.6 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 164.6/284.2 MB 38.6 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 166.3/284.2 MB 36.4 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 168.0/284.2 MB 36.4 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 169.6/284.2 MB 36.4 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 170.8/284.2 MB 36.4 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 173.0/284.2 MB 36.3 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 174.8/284.2 MB 36.3 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 176.3/284.2 MB 36.4 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 178.0/284.2 MB 36.4 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 179.5/284.2 MB 34.4 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 181.2/284.2 MB 36.4 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 182.9/284.2 MB 36.4 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 184.6/284.2 MB 34.4 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 186.3/284.2 MB 36.3 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 187.8/284.2 MB 34.4 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 189.5/284.2 MB 36.4 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 191.3/284.2 MB 36.4 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 193.1/284.2 MB 36.4 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 194.7/284.2 MB 36.4 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 196.4/284.2 MB 36.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 198.1/284.2 MB 36.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 199.7/284.2 MB 36.3 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 201.4/284.2 MB 36.3 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 202.8/284.2 MB 36.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 204.9/284.2 MB 36.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 206.5/284.2 MB 36.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 208.3/284.2 MB 36.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 209.9/284.2 MB 36.4 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 211.6/284.2 MB 36.4 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 213.2/284.2 MB 36.3 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 215.0/284.2 MB 36.3 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 216.6/284.2 MB 36.4 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 218.2/284.2 MB 34.4 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 219.8/284.2 MB 34.4 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 221.1/284.2 MB 36.4 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 223.3/284.2 MB 36.4 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 224.9/284.2 MB 34.4 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 226.8/284.2 MB 36.3 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 228.4/284.2 MB 36.3 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 230.0/284.2 MB 36.4 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 231.7/284.2 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 233.7/284.2 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 235.2/284.2 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 236.8/284.2 MB 34.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 238.6/284.2 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 240.2/284.2 MB 38.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 241.4/284.2 MB 36.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 243.2/284.2 MB 36.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 245.0/284.2 MB 36.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 246.8/284.2 MB 36.3 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 248.4/284.2 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 250.1/284.2 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 251.9/284.2 MB 38.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 253.6/284.2 MB 38.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 255.1/284.2 MB 36.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 256.9/284.2 MB 36.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 258.3/284.2 MB 36.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 259.9/284.2 MB 36.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 261.1/284.2 MB 36.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 263.2/284.2 MB 34.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 264.8/284.2 MB 34.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 266.4/284.2 MB 34.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 268.0/284.2 MB 34.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 269.9/284.2 MB 34.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 271.3/284.2 MB 34.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 272.7/284.2 MB 36.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 274.8/284.2 MB 36.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 276.5/284.2 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  278.4/284.2 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  279.9/284.2 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  281.7/284.2 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  283.4/284.2 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  284.2/284.2 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  284.2/284.2 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  284.2/284.2 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  284.2/284.2 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  284.2/284.2 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  284.2/284.2 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  284.2/284.2 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  284.2/284.2 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  284.2/284.2 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  284.2/284.2 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  284.2/284.2 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  284.2/284.2 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 284.2/284.2 MB 8.7 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.2.0-cp311-cp311-win_amd64.whl (938 kB)\n",
      "   ---------------------------------------- 0.0/938.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 938.7/938.7 kB 61.9 MB/s eta 0:00:00\n",
      "Downloading tensorflow_hub-0.14.0-py2.py3-none-any.whl (90 kB)\n",
      "   ---------------------------------------- 0.0/90.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 90.3/90.3 kB ? eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Downloading murmurhash-1.0.10-cp311-cp311-win_amd64.whl (25 kB)\n",
      "Downloading pathy-0.10.2-py3-none-any.whl (48 kB)\n",
      "   ---------------------------------------- 0.0/48.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 48.9/48.9 kB ? eta 0:00:00\n",
      "Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
      "   ---------------------------------------- 0.0/122.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 122.3/122.3 kB ? eta 0:00:00\n",
      "Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 57.0/57.0 kB ? eta 0:00:00\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp311-cp311-win_amd64.whl (479 kB)\n",
      "   ---------------------------------------- 0.0/479.7 kB ? eta -:--:--\n",
      "   ----------------------------- ---------- 358.4/479.7 kB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 479.7/479.7 kB 7.5 MB/s eta 0:00:00\n",
      "Downloading thinc-8.1.12-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.2/1.5 MB 7.3 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.6/1.5 MB 8.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 8.5 MB/s eta 0:00:00\n",
      "Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "   ---------------------------------------- 0.0/130.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 130.2/130.2 kB 7.5 MB/s eta 0:00:00\n",
      "Downloading blis-0.7.11-cp311-cp311-win_amd64.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.7/6.6 MB 15.8 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 1.6/6.6 MB 17.5 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.6/6.6 MB 18.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.8/6.6 MB 20.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.4/6.6 MB 23.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.6/6.6 MB 24.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 22.3 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.3-py3-none-any.whl (34 kB)\n",
      "Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Downloading h5py-3.9.0-cp311-cp311-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 1.3/2.7 MB 40.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.7 MB 33.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 33.8 MB/s eta 0:00:00\n",
      "Downloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 1.6/1.7 MB 48.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 36.1 MB/s eta 0:00:00\n",
      "Downloading libclang-16.0.6-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.8/24.4 MB 37.0 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 3.5/24.4 MB 37.3 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 5.1/24.4 MB 36.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 6.8/24.4 MB 36.5 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 8.4/24.4 MB 38.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 10.0/24.4 MB 35.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 11.6/24.4 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 13.3/24.4 MB 34.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 15.0/24.4 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 16.7/24.4 MB 34.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 18.4/24.4 MB 34.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 20.0/24.4 MB 34.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.8/24.4 MB 34.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.5/24.4 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.4 MB 36.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.4 MB 36.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.4 MB 36.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.4/24.4 MB 24.2 MB/s eta 0:00:00\n",
      "Downloading tensorboard-2.14.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 1.5/5.5 MB 48.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.3/5.5 MB 34.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.6/5.5 MB 36.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 32.0 MB/s eta 0:00:00\n",
      "Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
      "   ---------------------------------------- 0.0/440.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 440.7/440.7 kB 26.9 MB/s eta 0:00:00\n",
      "Downloading Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "   ---------------------------------------- 0.0/94.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 94.2/94.2 kB 5.6 MB/s eta 0:00:00\n",
      "Downloading tensorboard_data_server-0.7.1-py3-none-any.whl (2.4 kB)\n",
      "Downloading werkzeug-2.3.7-py3-none-any.whl (242 kB)\n",
      "   ---------------------------------------- 0.0/242.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 242.2/242.2 kB ? eta 0:00:00\n",
      "Building wheels for collected packages: evals, spacy-universal-sentence-encoder, wrapt\n",
      "  Building editable for evals (pyproject.toml): started\n",
      "  Building editable for evals (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for evals: filename=evals-1.0.3.post1-0.editable-py3-none-any.whl size=4971 sha256=f89f8f460bb38e638318520d891fb67d9a0cb81b6b6cbd73b8d2a339b6a859dc\n",
      "  Stored in directory: C:\\Users\\Mark\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-vbftwkns\\wheels\\21\\9b\\5f\\955a0fd905ea424144a3217ef64f9aacf3ceb73381d6f29710\n",
      "  Building wheel for spacy-universal-sentence-encoder (setup.py): started\n",
      "  Building wheel for spacy-universal-sentence-encoder (setup.py): finished with status 'done'\n",
      "  Created wheel for spacy-universal-sentence-encoder: filename=spacy_universal_sentence_encoder-0.4.6-py3-none-any.whl size=16574 sha256=99369031b846ee2a2b6d45d151ed50d4cbb85c522e338c95dfc200d8f6fa89eb\n",
      "  Stored in directory: c:\\users\\mark\\appdata\\local\\pip\\cache\\wheels\\1f\\52\\3f\\62024dc821401e2de809ec48ba82ad9bd096de2389a3675d23\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.14.1-cp311-cp311-win_amd64.whl size=21667 sha256=1e00f68b371f14eb2aad59ba3abeebb8fd93b909c94ecd850ea734417c80d87b\n",
      "  Stored in directory: c:\\users\\mark\\appdata\\local\\pip\\cache\\wheels\\eb\\b6\\fa\\5ab6f4107cad63fa04c54ad78d75bb7035119bdd4f751df5ae\n",
      "Successfully built evals spacy-universal-sentence-encoder wrapt\n",
      "Installing collected packages: libclang, flatbuffers, cymem, wrapt, werkzeug, wasabi, tensorflow-io-gcs-filesystem, tensorflow-hub, tensorflow-estimator, tensorboard-data-server, spacy-loggers, spacy-legacy, smart-open, rapidfuzz, pyarrow, platformdirs, opt-einsum, oauthlib, murmurhash, ml-dtypes, markdown, langcodes, keras, h5py, google-pasta, gast, catalogue, blis, astunparse, absl-py, typer, srsly, requests-oauthlib, preshed, jiwer, pathy, google-auth-oauthlib, confection, thinc, tensorboard, tensorflow-intel, spacy, tensorflow, spacy-universal-sentence-encoder, evals\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 13.0.0\n",
      "    Uninstalling pyarrow-13.0.0:\n",
      "      Successfully uninstalled pyarrow-13.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\Mark\\\\anaconda3\\\\envs\\\\testenv\\\\Lib\\\\site-packages\\\\~yarrow\\\\arrow.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# openai evals uses git-lfs, but installation may be system-specific\n",
    "#!git lfs install\n",
    "\n",
    "# get a local copy of evals (and if one already exists, nuke it first)\n",
    "try:\n",
    "    !rm -rf evals\n",
    "finally:\n",
    "    !git clone https://github.com/MHLoppy/evals.git\n",
    "\n",
    "# complete the remaining setup steps\n",
    "!cd evals\n",
    "!git lfs fetch --all\n",
    "!git lfs pull\n",
    "%pip install -e evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use/Run evals\n",
    "\n",
    "(As an aside, while using evals without both magic commands and manual file creation is possible (see https://medium.com/@sergioli/evaluating-chatgpt-using-openai-evals-7ca85c0ad139), it's comparatively more complex.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note how the lists here are appended with an extra set of quotes\n",
    "#   this is being done because we're running shell commands that need quotes\n",
    "\n",
    "# Construct list of sample file paths\n",
    "sample_paths = []\n",
    "for index, chunk in enumerate(wd_list):\n",
    "    sample_path = os.path.join(cwd, \"eval_samples\", f\"new_samples_{index}.jsonl\")\n",
    "    sample_paths.append(f\"{sample_path}\")\n",
    "\n",
    "# Construct list of record file paths\n",
    "record_paths = []\n",
    "for index, chunk in enumerate(wd_list):\n",
    "    record_path = os.path.join(cwd, \"eval_records\", f\"eval_record_{index}.jsonl\")\n",
    "    record_paths.append(f'\\\"{record_path}\\\"')\n",
    "\n",
    "# Construct list of log file paths\n",
    "log_paths = []\n",
    "for index, chunk in enumerate(wd_list):\n",
    "    log_path = os.path.join(cwd, \"eval_logs\", f\"eval_log_{index}.jsonl\")\n",
    "    log_paths.append(f'\\\"{log_path}\\\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:10<00:00, 10.39s/it]\n",
      "100%|██████████| 1/1 [00:10<00:00, 10.39s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:14<00:00, 14.67s/it]\n",
      "100%|██████████| 1/1 [00:14<00:00, 14.67s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:15<00:00, 15.42s/it]\n",
      "100%|██████████| 1/1 [00:15<00:00, 15.42s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:33<00:00, 33.68s/it]\n",
      "100%|██████████| 1/1 [00:33<00:00, 33.68s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:27<00:00, 27.12s/it]\n",
      "100%|██████████| 1/1 [00:27<00:00, 27.12s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:20<00:00, 20.44s/it]\n",
      "100%|██████████| 1/1 [00:20<00:00, 20.44s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:29<00:00, 29.80s/it]\n",
      "100%|██████████| 1/1 [00:29<00:00, 29.80s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:35<00:00, 35.36s/it]\n",
      "100%|██████████| 1/1 [00:35<00:00, 35.36s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:14<00:00, 14.27s/it]\n",
      "100%|██████████| 1/1 [00:14<00:00, 14.27s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:23<00:00, 23.22s/it]\n",
      "100%|██████████| 1/1 [00:23<00:00, 23.22s/it]\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "samples_file_path = os.path.join(cwd, \"evals\", \"evals\", \"registry\", \"data\", \"coqa\", \"samples.jsonl\")\n",
    "\n",
    "# Run chunked evals\n",
    "for index, chunk in enumerate(wd_list):\n",
    "\n",
    "    # Update the samples file programmatically each iteration\n",
    "    shutil.copy(sample_paths[index], samples_file_path)\n",
    "\n",
    "    # Run the evaluation and save the results (records) and log file as specified\n",
    "    record = record_paths[index]\n",
    "    log = log_paths[index]\n",
    "    !oaieval gpt-4 coqa-fact --record_path $record --log_to_file $log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>accepted_answer_id</th>\n",
       "      <th>view_count</th>\n",
       "      <th>tags</th>\n",
       "      <th>answer_count</th>\n",
       "      <th>question_score</th>\n",
       "      <th>answer_score</th>\n",
       "      <th>stackoverflow_answer</th>\n",
       "      <th>stripped_title</th>\n",
       "      <th>stripped_body</th>\n",
       "      <th>stripped_stackoverflow_answer</th>\n",
       "      <th>gpt-3.5-turbo-16k_answer</th>\n",
       "      <th>stripped_gpt-3.5-turbo-16k_answer</th>\n",
       "      <th>eval_choice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196461</td>\n",
       "      <td>71067715</td>\n",
       "      <td>CRA error. Html Webpack Plugin: Error: Child c...</td>\n",
       "      <td>&lt;p&gt;I create a default react app with &lt;code&gt;npx...</td>\n",
       "      <td>71086154</td>\n",
       "      <td>221</td>\n",
       "      <td>node.js|reactjs|webpack|create-react-app</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;p&gt;Solution: install &lt;strong&gt;react-scripts@4.0...</td>\n",
       "      <td>CRA error. Html Webpack Plugin: Error: Child c...</td>\n",
       "      <td>I create a default react app with npx create-r...</td>\n",
       "      <td>Solution: install react-scripts@4.0.3. If you ...</td>\n",
       "      <td>It seems like you are encountering an error re...</td>\n",
       "      <td>It seems like you are encountering an error re...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>243249</td>\n",
       "      <td>71098325</td>\n",
       "      <td>Hyperledger Fabric | Orderer PODs keeps restar...</td>\n",
       "      <td>&lt;p&gt;I'm running Hyperledger Fabric network in A...</td>\n",
       "      <td>71168925</td>\n",
       "      <td>239</td>\n",
       "      <td>azure|kubernetes|hyperledger-fabric|raft</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;p&gt;Turns out my WAL logs directory was deleted...</td>\n",
       "      <td>Hyperledger Fabric | Orderer PODs keeps restar...</td>\n",
       "      <td>I'm running Hyperledger Fabric network in Azur...</td>\n",
       "      <td>Turns out my WAL logs directory was deleted. A...</td>\n",
       "      <td>The error message suggests that the raft log o...</td>\n",
       "      <td>The error message suggests that the raft log o...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>375295</td>\n",
       "      <td>73155785</td>\n",
       "      <td>Grouping character strings</td>\n",
       "      <td>&lt;p&gt;I have 3 text :&lt;/p&gt;\\n&lt;ul&gt;\\n&lt;li&gt;Simple test ...</td>\n",
       "      <td>73155978</td>\n",
       "      <td>21</td>\n",
       "      <td>php|regex</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;p&gt;If you want to match the square brackets, y...</td>\n",
       "      <td>Grouping character strings</td>\n",
       "      <td>I have 3 text :\\n\\nSimple test 1 [https://www....</td>\n",
       "      <td>If you want to match the square brackets, you ...</td>\n",
       "      <td>To achieve the desired grouping, you can use t...</td>\n",
       "      <td>To achieve the desired grouping, you can use t...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>254120</td>\n",
       "      <td>71900912</td>\n",
       "      <td>Convert array from spreadsheet into associativ...</td>\n",
       "      <td>&lt;p&gt;I've been having difficulty visualizing how...</td>\n",
       "      <td>71901148</td>\n",
       "      <td>96</td>\n",
       "      <td>php|arrays|multidimensional-array</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;p&gt;This is a common issue with spreadsheets an...</td>\n",
       "      <td>Convert array from spreadsheet into associativ...</td>\n",
       "      <td>I've been having difficulty visualizing how to...</td>\n",
       "      <td>This is a common issue with spreadsheets and C...</td>\n",
       "      <td>Yes, the `array_combine` function can be used ...</td>\n",
       "      <td>Yes, the `array_combine` function can be used ...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>361337</td>\n",
       "      <td>71269489</td>\n",
       "      <td>ID components may not include unresolved token...</td>\n",
       "      <td>&lt;p&gt;I am trying to use a CfnParameter in the AW...</td>\n",
       "      <td>71313692</td>\n",
       "      <td>1213</td>\n",
       "      <td>python|amazon-web-services|amazon-cloudformati...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;p&gt;Managed to resolve by using --context inste...</td>\n",
       "      <td>ID components may not include unresolved token...</td>\n",
       "      <td>I am trying to use a CfnParameter in the AWS P...</td>\n",
       "      <td>Managed to resolve by using --context instead ...</td>\n",
       "      <td>The error message suggests that you are trying...</td>\n",
       "      <td>The error message suggests that you are trying...</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index        id                                              title  \\\n",
       "0  196461  71067715  CRA error. Html Webpack Plugin: Error: Child c...   \n",
       "1  243249  71098325  Hyperledger Fabric | Orderer PODs keeps restar...   \n",
       "2  375295  73155785                         Grouping character strings   \n",
       "3  254120  71900912  Convert array from spreadsheet into associativ...   \n",
       "4  361337  71269489  ID components may not include unresolved token...   \n",
       "\n",
       "                                                body  accepted_answer_id  \\\n",
       "0  <p>I create a default react app with <code>npx...            71086154   \n",
       "1  <p>I'm running Hyperledger Fabric network in A...            71168925   \n",
       "2  <p>I have 3 text :</p>\\n<ul>\\n<li>Simple test ...            73155978   \n",
       "3  <p>I've been having difficulty visualizing how...            71901148   \n",
       "4  <p>I am trying to use a CfnParameter in the AW...            71313692   \n",
       "\n",
       "   view_count                                               tags  \\\n",
       "0         221           node.js|reactjs|webpack|create-react-app   \n",
       "1         239           azure|kubernetes|hyperledger-fabric|raft   \n",
       "2          21                                          php|regex   \n",
       "3          96                  php|arrays|multidimensional-array   \n",
       "4        1213  python|amazon-web-services|amazon-cloudformati...   \n",
       "\n",
       "   answer_count  question_score  answer_score  \\\n",
       "0             1               0             0   \n",
       "1             1               0             0   \n",
       "2             1               0             2   \n",
       "3             1               0             1   \n",
       "4             1               0             1   \n",
       "\n",
       "                                stackoverflow_answer  \\\n",
       "0  <p>Solution: install <strong>react-scripts@4.0...   \n",
       "1  <p>Turns out my WAL logs directory was deleted...   \n",
       "2  <p>If you want to match the square brackets, y...   \n",
       "3  <p>This is a common issue with spreadsheets an...   \n",
       "4  <p>Managed to resolve by using --context inste...   \n",
       "\n",
       "                                      stripped_title  \\\n",
       "0  CRA error. Html Webpack Plugin: Error: Child c...   \n",
       "1  Hyperledger Fabric | Orderer PODs keeps restar...   \n",
       "2                         Grouping character strings   \n",
       "3  Convert array from spreadsheet into associativ...   \n",
       "4  ID components may not include unresolved token...   \n",
       "\n",
       "                                       stripped_body  \\\n",
       "0  I create a default react app with npx create-r...   \n",
       "1  I'm running Hyperledger Fabric network in Azur...   \n",
       "2  I have 3 text :\\n\\nSimple test 1 [https://www....   \n",
       "3  I've been having difficulty visualizing how to...   \n",
       "4  I am trying to use a CfnParameter in the AWS P...   \n",
       "\n",
       "                       stripped_stackoverflow_answer  \\\n",
       "0  Solution: install react-scripts@4.0.3. If you ...   \n",
       "1  Turns out my WAL logs directory was deleted. A...   \n",
       "2  If you want to match the square brackets, you ...   \n",
       "3  This is a common issue with spreadsheets and C...   \n",
       "4  Managed to resolve by using --context instead ...   \n",
       "\n",
       "                            gpt-3.5-turbo-16k_answer  \\\n",
       "0  It seems like you are encountering an error re...   \n",
       "1  The error message suggests that the raft log o...   \n",
       "2  To achieve the desired grouping, you can use t...   \n",
       "3  Yes, the `array_combine` function can be used ...   \n",
       "4  The error message suggests that you are trying...   \n",
       "\n",
       "                   stripped_gpt-3.5-turbo-16k_answer eval_choice  \n",
       "0  It seems like you are encountering an error re...           A  \n",
       "1  The error message suggests that the raft log o...           C  \n",
       "2  To achieve the desired grouping, you can use t...           A  \n",
       "3  Yes, the `array_combine` function can be used ...           A  \n",
       "4  The error message suggests that you are trying...           B  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obed's answer reading code\n",
    "#   (because by his evaluation, the metric disagrees with the written response ~10% of the time)\n",
    "import re\n",
    "def get_answer_from_response(text):\n",
    "    \"\"\"Parses the output text for the evaluation choice.\"\"\"\n",
    "    \n",
    "    last_letter = text[-1]\n",
    "    if last_letter not in ['A', 'B', 'C', 'D', 'E']:\n",
    "        matches = re.findall('\\((.*?)\\)', text)\n",
    "        return matches[-1] if matches else None      \n",
    "    return last_letter\n",
    "\n",
    "\n",
    "# Iterate through our evals results\n",
    "for index, chunk in enumerate(wd_list):\n",
    "\n",
    "    # Update our record path each iteration\n",
    "    record_path = record_paths[index]\n",
    "\n",
    "    # Empty lists for us to store stuff in, to later add to the chunk's dataframe\n",
    "    answer_list = []\n",
    "    provided_answer_list = []\n",
    "    sampled_list = []\n",
    "    metric_list = []\n",
    "\n",
    "    # Open the record (.jsonl) for this iteration\n",
    "    with open(record_path) as f:\n",
    "        # Skip the first two lines (they're just metadata about the query and response)\n",
    "        for _ in range(2):\n",
    "            next(f)\n",
    "\n",
    "        # Iterate through the rest of the file line-by-line\n",
    "        for line in f:\n",
    "            eval_line = json.loads(line)\n",
    "\n",
    "            # process the \"sampling\" half of each evaluation response\n",
    "            if eval_line[\"type\"] == \"sampling\":\n",
    "\n",
    "                # Extract the response from the answer received\n",
    "                answer = eval_line[\"data\"][\"sampled\"][0]\n",
    "                extr_choice = get_answer_from_response(answer)\n",
    "\n",
    "                # Add the extracted response and the raw response to our lists\n",
    "                answer_list.append(extr_choice)\n",
    "                sampled_list.append(eval_line)\n",
    "\n",
    "            # Process the \"metric\" half of each evaluation response\n",
    "            elif eval_line[\"type\"] == \"metric\":\n",
    "\n",
    "                # Also pull evals' self-reported response\n",
    "                og_choice = eval_line[\"data\"][\"choice\"]\n",
    "\n",
    "                # Add that and the raw response to our lists\n",
    "                provided_answer_list.append(og_choice)\n",
    "                metric_list.append(eval_line)\n",
    "\n",
    "    # Add the populated lists into our chunk's dataframe\n",
    "    chunk[\"original_eval_choice\"] = provided_answer_list\n",
    "    chunk[\"extracted_eval_choice\"] = answer_list\n",
    "    chunk[\"eval_full_sampled\"] = sampled_list\n",
    "    chunk[\"eval_full_metric\"] = metric_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine each of the chunked dataframes, export to CSV\n",
    "combined = pd.concat(wd_list)\n",
    "combined.to_csv(\"dataset_results.csv\")\n",
    "\n",
    "# Preview the result\n",
    "combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra bit for estimating the token count of the evals prompt (since this will count against our token limit!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243\n"
     ]
    }
   ],
   "source": [
    "# Estimate token count of evals prompt\n",
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "text = \"\\n\\nCompare the factual content of the submitted answer with the expert answer. Ignore any differences in style, grammar, or punctuation.\\nThe submitted answer may either be a subset or superset of the expert answer, or it may conflict with it. Determine which case applies. Answer the question by selecting one of the following options:\\n(A) The submitted answer is a subset of the expert answer and is fully consistent with it.\\n(B) The submitted answer is a superset of the expert answer and is fully consistent with it.\\n(C) The submitted answer contains all the same details as the expert answer.\\n(D) There is a disagreement between the submitted answer and the expert answer.\\n(E) The answers differ, but these differences don't matter from the perspective of factuality.\\n\\nFirst, write out in a step by step manner your reasoning to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset. Then print only a single choice from \\\"A\\\" or \\\"B\\\" or \\\"C\\\" or \\\"D\\\" or \\\"E\\\" (without quotes or punctuation) on its own line corresponding to the correct answer. At the end, repeat just the answer by itself on a new line.\\n\\nReasoning:\"\n",
    "\n",
    "tokens = len(encoding.encode(text, disallowed_special=()))\n",
    "\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
