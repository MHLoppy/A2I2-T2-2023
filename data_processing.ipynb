{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset to dataframe\n",
    "\n",
    "Note that the dataset might be coming from BigQuery or from a query on the local database (created from the SE Data Dump). The two data sources should be interchangeable in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved copy of dataset loaded from local disk.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATASET_FILE = \"saved_dataset.csv\"      # The file name of the saved dataset (saved on / loaded from local disk)\n",
    "cwd = Path().absolute()                 # Current working directory (note: possibly different from execution directory)\n",
    "\n",
    "# Load a saved copy of the dataset from local disk (if it exists)\n",
    "try:\n",
    "    dataset_path = os.path.join(cwd, DATASET_FILE)\n",
    "    results = pd.read_csv(dataset_path)\n",
    "    results = results.astype({\"creation_date\": \"datetime64[ns]\"})\n",
    "    print(\"Saved copy of dataset loaded from local disk.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Saved dataset not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cull / filter dataset\n",
    "\n",
    "For the demo we're just arbitrarily culling the size of the dataset to make it more manageable, but you could also filter for other reasons such as focusing on a specific tag, or sampling based on answer and/or question scores.\n",
    "\n",
    "A pandas dataframe can be sampled either:\n",
    "* Using a fractional value: e.g., ``.sample(frac=0.01)`` will result in a number of samples equivalent to 1% of the dataset.\n",
    "* Using an integer value: e.g., ``.sample(n=1000)`` will result in 1000 samples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions in currently selected filtered dataset: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>accepted_answer_id</th>\n",
       "      <th>view_count</th>\n",
       "      <th>tags</th>\n",
       "      <th>answer_count</th>\n",
       "      <th>question_score</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>answer_score</th>\n",
       "      <th>stackoverflow_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>408072</th>\n",
       "      <td>72674706</td>\n",
       "      <td>What is the REACT QUERY way to reuse global st...</td>\n",
       "      <td>&lt;p&gt;In redux/context api, we used to fetch the ...</td>\n",
       "      <td>72676239</td>\n",
       "      <td>9158</td>\n",
       "      <td>&lt;reactjs&gt;&lt;react-query&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-19 06:28:02</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;p&gt;You can just call &lt;code&gt;useQuery&lt;/code&gt; whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619720</th>\n",
       "      <td>74458975</td>\n",
       "      <td>how to make Localization with Getx if there is...</td>\n",
       "      <td>&lt;p&gt;if the text like this:&lt;/p&gt;&amp;#xA;&lt;pre&gt;&lt;code&gt; ...</td>\n",
       "      <td>74459510</td>\n",
       "      <td>2019</td>\n",
       "      <td>&lt;flutter&gt;&lt;flutter-getx&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-11-16 10:36:18</td>\n",
       "      <td>10</td>\n",
       "      <td>&lt;p&gt;The documentation of GetX explains well how...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693780</th>\n",
       "      <td>75121109</td>\n",
       "      <td>React Query invalidateQueries not updating the UI</td>\n",
       "      <td>&lt;p&gt;My UI is not updating on the creation of a ...</td>\n",
       "      <td>75122016</td>\n",
       "      <td>492</td>\n",
       "      <td>&lt;reactjs&gt;&lt;next.js&gt;&lt;react-query&gt;&lt;supabase&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-14 20:44:38</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;p&gt;You are instantiating &lt;code&gt;QueryClient&lt;/co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118789</th>\n",
       "      <td>70326274</td>\n",
       "      <td>jQuery find empty inputs for particular form</td>\n",
       "      <td>&lt;p&gt;I am trying to find and set values on empty...</td>\n",
       "      <td>70326361</td>\n",
       "      <td>22</td>\n",
       "      <td>&lt;jquery&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-12-12 18:01:41</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;p&gt;You need to look inside the form instance u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66379</th>\n",
       "      <td>69913488</td>\n",
       "      <td>How to authenticate a ldap user from any ldap ...</td>\n",
       "      <td>&lt;p&gt;I want to implement ldap authentication in ...</td>\n",
       "      <td>70022969</td>\n",
       "      <td>1952</td>\n",
       "      <td>&lt;spring-boot&gt;&lt;spring-security&gt;&lt;active-director...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-11-10 12:44:34</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;p&gt;OK. So after spending lot of times I got so...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                              title  \\\n",
       "408072  72674706  What is the REACT QUERY way to reuse global st...   \n",
       "619720  74458975  how to make Localization with Getx if there is...   \n",
       "693780  75121109  React Query invalidateQueries not updating the UI   \n",
       "118789  70326274       jQuery find empty inputs for particular form   \n",
       "66379   69913488  How to authenticate a ldap user from any ldap ...   \n",
       "\n",
       "                                                     body  accepted_answer_id  \\\n",
       "408072  <p>In redux/context api, we used to fetch the ...            72676239   \n",
       "619720  <p>if the text like this:</p>&#xA;<pre><code> ...            74459510   \n",
       "693780  <p>My UI is not updating on the creation of a ...            75122016   \n",
       "118789  <p>I am trying to find and set values on empty...            70326361   \n",
       "66379   <p>I want to implement ldap authentication in ...            70022969   \n",
       "\n",
       "        view_count                                               tags  \\\n",
       "408072        9158                             <reactjs><react-query>   \n",
       "619720        2019                            <flutter><flutter-getx>   \n",
       "693780         492          <reactjs><next.js><react-query><supabase>   \n",
       "118789          22                                           <jquery>   \n",
       "66379         1952  <spring-boot><spring-security><active-director...   \n",
       "\n",
       "        answer_count  question_score       creation_date  answer_score  \\\n",
       "408072             1               1 2022-06-19 06:28:02             3   \n",
       "619720             1               2 2022-11-16 10:36:18            10   \n",
       "693780             1               1 2023-01-14 20:44:38             2   \n",
       "118789             1               0 2021-12-12 18:01:41             0   \n",
       "66379              1               1 2021-11-10 12:44:34             0   \n",
       "\n",
       "                                     stackoverflow_answer  \n",
       "408072  <p>You can just call <code>useQuery</code> whe...  \n",
       "619720  <p>The documentation of GetX explains well how...  \n",
       "693780  <p>You are instantiating <code>QueryClient</co...  \n",
       "118789  <p>You need to look inside the form instance u...  \n",
       "66379   <p>OK. So after spending lot of times I got so...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly sample dataset\n",
    "fd_tiny = results.sample(frac=0.0001)\n",
    "fd_nano = results.sample(n=10)\n",
    "fd_1k   = results.sample(n=1000)\n",
    "fd_10k  = results.sample(n=10000)\n",
    "fd_100k = results.sample(n=100000)\n",
    "\n",
    "# Convenience: alias the filtered data so we can change it easily for later code\n",
    "wd = fd_nano\n",
    "\n",
    "# Dump info about the filtered result\n",
    "print(\"Number of questions in currently selected filtered dataset:\", len(wd))\n",
    "wd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strip HTML from filtered dataset\n",
    "\n",
    "We decided we would strip HTML and use this \"stripped\" version as our default for evaluations. The stripped text is appended as a separate column in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "# Separate out the text columns we want for convenience\n",
    "titles  = wd[\"title\"]\n",
    "bodies  = wd[\"body\"]\n",
    "answers = wd[\"stackoverflow_answer\"]\n",
    "\n",
    "# Sanity check (surely this will always be true, but *just in case*)\n",
    "if len(titles) == len(bodies) and len(titles) == len(answers):\n",
    "    pass\n",
    "else:\n",
    "    raise ValueError(\"columns are different lengths!\")\n",
    "\n",
    "# Create new lists to store HTML-stripped versions of text\n",
    "s_titles  = []\n",
    "s_bodies  = []\n",
    "s_answers = []\n",
    "\n",
    "# Iterating is slow(!) but comparatively easy to understand (modified Harvey approach)\n",
    "for idx, *row in wd.itertuples():\n",
    "    \n",
    "    # Strip HTML from title, question, and answer; save to lists\n",
    "    s_titles.append(soup(titles[idx], \"html.parser\").get_text())\n",
    "    s_bodies.append(soup(bodies[idx], \"html.parser\").get_text())\n",
    "    s_answers.append(soup(answers[idx], \"html.parser\").get_text())\n",
    "\n",
    "# Add the populated lists into our dataframe\n",
    "wd[\"stripped_title\"] = s_titles\n",
    "wd[\"stripped_body\"] = s_bodies\n",
    "wd[\"stripped_stackoverflow_answer\"] = s_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install OpenAI library and Configure OpenAI API Key\n",
    "\n",
    "Currently configured using secrets.json located at the root directory. An alternative method (which would require code changes) would be to read the system's environment variable.\n",
    "\n",
    "Key can be generated from: https://platform.openai.com/account/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (0.28.0)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from openai) (3.8.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from requests>=2.20->openai) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from requests>=2.20->openai) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from requests>=2.20->openai) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from aiohttp->openai) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from tqdm->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install OpenAI\n",
    "%pip install openai\n",
    "import openai\n",
    "\n",
    "# Function to load OpenAI API key from file\n",
    "# https://stackoverflow.com/a/76148268\n",
    "def load_api_key(secrets_file=\"secrets.json\"):\n",
    "    with open(secrets_file) as f:\n",
    "        secrets = json.load(f)\n",
    "    return secrets[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Read and set our OpenAI API key\n",
    "api_key = load_api_key()\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataframe into chunks for batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>accepted_answer_id</th>\n",
       "      <th>view_count</th>\n",
       "      <th>tags</th>\n",
       "      <th>answer_count</th>\n",
       "      <th>question_score</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>answer_score</th>\n",
       "      <th>stackoverflow_answer</th>\n",
       "      <th>stripped_title</th>\n",
       "      <th>stripped_body</th>\n",
       "      <th>stripped_stackoverflow_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>408072</th>\n",
       "      <td>72674706</td>\n",
       "      <td>What is the REACT QUERY way to reuse global st...</td>\n",
       "      <td>&lt;p&gt;In redux/context api, we used to fetch the ...</td>\n",
       "      <td>72676239</td>\n",
       "      <td>9158</td>\n",
       "      <td>&lt;reactjs&gt;&lt;react-query&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-19 06:28:02</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;p&gt;You can just call &lt;code&gt;useQuery&lt;/code&gt; whe...</td>\n",
       "      <td>What is the REACT QUERY way to reuse global st...</td>\n",
       "      <td>In redux/context api, we used to fetch the dat...</td>\n",
       "      <td>You can just call useQuery wherever you want t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                              title  \\\n",
       "408072  72674706  What is the REACT QUERY way to reuse global st...   \n",
       "\n",
       "                                                     body  accepted_answer_id  \\\n",
       "408072  <p>In redux/context api, we used to fetch the ...            72676239   \n",
       "\n",
       "        view_count                    tags  answer_count  question_score  \\\n",
       "408072        9158  <reactjs><react-query>             1               1   \n",
       "\n",
       "             creation_date  answer_score  \\\n",
       "408072 2022-06-19 06:28:02             3   \n",
       "\n",
       "                                     stackoverflow_answer  \\\n",
       "408072  <p>You can just call <code>useQuery</code> whe...   \n",
       "\n",
       "                                           stripped_title  \\\n",
       "408072  What is the REACT QUERY way to reuse global st...   \n",
       "\n",
       "                                            stripped_body  \\\n",
       "408072  In redux/context api, we used to fetch the dat...   \n",
       "\n",
       "                            stripped_stackoverflow_answer  \n",
       "408072  You can just call useQuery wherever you want t...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Split into n parts (in our case 10) in a list\n",
    "wd_list = np.array_split(wd, 10)\n",
    "\n",
    "# Check the result of the first chunk\n",
    "wd_list[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get GPT answers to SO questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tenacity in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (8.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Used for rate limit handling with OpenAI API\n",
    "%pip install tenacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from tiktoken) (2023.8.8)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Used to estimate token counts\n",
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 entries were skipped due to token length.\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff\n",
    "\n",
    "MODEL_NAME = \"gpt-4\"\n",
    "ENCODING = tiktoken.encoding_for_model(MODEL_NAME)\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def completion_with_backoff(**kwargs):\n",
    "    response = openai.ChatCompletion.create(**kwargs)\n",
    "    return response\n",
    "\n",
    "def chat_format(question):\n",
    "    \"\"\"Insert the full prompt into chat format.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "skipped = 0\n",
    "\n",
    "for chunk in wd_list:\n",
    "    GPT_answers = []\n",
    "    GPT_finished = []\n",
    "    full_responses = []\n",
    "\n",
    "    for idx, row in chunk.iterrows():\n",
    "\n",
    "        title_and_question = row[\"stripped_title\"] + \"\\n\\n\" + row[\"stripped_body\"]\n",
    "        SO_text = title_and_question + row[\"stripped_stackoverflow_answer\"]\n",
    "\n",
    "        # Estimate tokens for SO T+Q+A, and skip anything too long\n",
    "        if len(ENCODING.encode(SO_text)) <= 4000:\n",
    "\n",
    "            # Get the response from GPT\n",
    "            prompt = chat_format(title_and_question)\n",
    "            GPT_answer = completion_with_backoff(model=MODEL_NAME, messages=prompt, temperature=0, max_tokens=2000)\n",
    "            extracted_answer = GPT_answer.choices[0].message.content\n",
    "\n",
    "            # Check if the GPT response completed or terminated early (because e.g. hit token limit)\n",
    "            if GPT_answer.choices[0].finish_reason == \"stop\":\n",
    "                finished = True\n",
    "            else:\n",
    "                finished = False\n",
    "\n",
    "        else:\n",
    "            skipped += 1\n",
    "            extracted_answer = None\n",
    "            GPT_answer = None\n",
    "            finished = False\n",
    "\n",
    "        # Add to our lists\n",
    "        GPT_answers.append(extracted_answer)\n",
    "        GPT_finished.append(finished)\n",
    "        full_responses.append(GPT_answer)\n",
    "\n",
    "    # Add answers back into the chunk dataframe\n",
    "    COL_NAME = f\"{MODEL_NAME}_answer\"\n",
    "    chunk[COL_NAME] = GPT_answers\n",
    "    chunk[\"GPT_finished\"] = GPT_finished\n",
    "    chunk[\"full_GPT_response\"] = full_responses\n",
    "\n",
    "# Note how many questions have been skipped for length reasons\n",
    "print(skipped, \"entries were skipped due to token length.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>accepted_answer_id</th>\n",
       "      <th>view_count</th>\n",
       "      <th>tags</th>\n",
       "      <th>answer_count</th>\n",
       "      <th>question_score</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>answer_score</th>\n",
       "      <th>stackoverflow_answer</th>\n",
       "      <th>stripped_title</th>\n",
       "      <th>stripped_body</th>\n",
       "      <th>stripped_stackoverflow_answer</th>\n",
       "      <th>gpt-4_answer</th>\n",
       "      <th>GPT_finished</th>\n",
       "      <th>full_GPT_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>408072</th>\n",
       "      <td>72674706</td>\n",
       "      <td>What is the REACT QUERY way to reuse global st...</td>\n",
       "      <td>&lt;p&gt;In redux/context api, we used to fetch the ...</td>\n",
       "      <td>72676239</td>\n",
       "      <td>9158</td>\n",
       "      <td>&lt;reactjs&gt;&lt;react-query&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-19 06:28:02</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;p&gt;You can just call &lt;code&gt;useQuery&lt;/code&gt; whe...</td>\n",
       "      <td>What is the REACT QUERY way to reuse global st...</td>\n",
       "      <td>In redux/context api, we used to fetch the dat...</td>\n",
       "      <td>You can just call useQuery wherever you want t...</td>\n",
       "      <td>React Query doesn't replace Redux or Context A...</td>\n",
       "      <td>True</td>\n",
       "      <td>{'id': 'chatcmpl-83Ku1F71nosfdcLHOuklckauIH42b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                              title  \\\n",
       "408072  72674706  What is the REACT QUERY way to reuse global st...   \n",
       "\n",
       "                                                     body  accepted_answer_id  \\\n",
       "408072  <p>In redux/context api, we used to fetch the ...            72676239   \n",
       "\n",
       "        view_count                    tags  answer_count  question_score  \\\n",
       "408072        9158  <reactjs><react-query>             1               1   \n",
       "\n",
       "             creation_date  answer_score  \\\n",
       "408072 2022-06-19 06:28:02             3   \n",
       "\n",
       "                                     stackoverflow_answer  \\\n",
       "408072  <p>You can just call <code>useQuery</code> whe...   \n",
       "\n",
       "                                           stripped_title  \\\n",
       "408072  What is the REACT QUERY way to reuse global st...   \n",
       "\n",
       "                                            stripped_body  \\\n",
       "408072  In redux/context api, we used to fetch the dat...   \n",
       "\n",
       "                            stripped_stackoverflow_answer  \\\n",
       "408072  You can just call useQuery wherever you want t...   \n",
       "\n",
       "                                             gpt-4_answer  GPT_finished  \\\n",
       "408072  React Query doesn't replace Redux or Context A...          True   \n",
       "\n",
       "                                        full_GPT_response  \n",
       "408072  {'id': 'chatcmpl-83Ku1F71nosfdcLHOuklckauIH42b...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check result of first chunk\n",
    "wd_list[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>accepted_answer_id</th>\n",
       "      <th>view_count</th>\n",
       "      <th>tags</th>\n",
       "      <th>answer_count</th>\n",
       "      <th>question_score</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>answer_score</th>\n",
       "      <th>stackoverflow_answer</th>\n",
       "      <th>stripped_title</th>\n",
       "      <th>stripped_body</th>\n",
       "      <th>stripped_stackoverflow_answer</th>\n",
       "      <th>gpt-4_answer</th>\n",
       "      <th>GPT_finished</th>\n",
       "      <th>full_GPT_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>650846</th>\n",
       "      <td>74737897</td>\n",
       "      <td>Why can't make command see my Python venv dire...</td>\n",
       "      <td>&lt;p&gt;I'm writing a &lt;code&gt;Makefile&lt;/code&gt; for my ...</td>\n",
       "      <td>74738195</td>\n",
       "      <td>247</td>\n",
       "      <td>&lt;python&gt;&lt;makefile&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-12-09 01:03:11</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;p&gt;Per Enis's linked answer in the comments, t...</td>\n",
       "      <td>Why can't make command see my Python venv dire...</td>\n",
       "      <td>I'm writing a Makefile for my project (using M...</td>\n",
       "      <td>Per Enis's linked answer in the comments, this...</td>\n",
       "      <td>The issue here is that each line in a Makefile...</td>\n",
       "      <td>True</td>\n",
       "      <td>{'id': 'chatcmpl-83Ky7DD3JGyn09N8naXSKr7HImNjn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                              title  \\\n",
       "650846  74737897  Why can't make command see my Python venv dire...   \n",
       "\n",
       "                                                     body  accepted_answer_id  \\\n",
       "650846  <p>I'm writing a <code>Makefile</code> for my ...            74738195   \n",
       "\n",
       "        view_count                tags  answer_count  question_score  \\\n",
       "650846         247  <python><makefile>             1               0   \n",
       "\n",
       "             creation_date  answer_score  \\\n",
       "650846 2022-12-09 01:03:11             0   \n",
       "\n",
       "                                     stackoverflow_answer  \\\n",
       "650846  <p>Per Enis's linked answer in the comments, t...   \n",
       "\n",
       "                                           stripped_title  \\\n",
       "650846  Why can't make command see my Python venv dire...   \n",
       "\n",
       "                                            stripped_body  \\\n",
       "650846  I'm writing a Makefile for my project (using M...   \n",
       "\n",
       "                            stripped_stackoverflow_answer  \\\n",
       "650846  Per Enis's linked answer in the comments, this...   \n",
       "\n",
       "                                             gpt-4_answer  GPT_finished  \\\n",
       "650846  The issue here is that each line in a Makefile...          True   \n",
       "\n",
       "                                        full_GPT_response  \n",
       "650846  {'id': 'chatcmpl-83Ky7DD3JGyn09N8naXSKr7HImNjn...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check result of last chun\n",
    "wd_list[9].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare JSONL file for evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip and save a copy of the GPT answer\n",
    "#   (so that the eval is on fair footing, with HTML tags removed from both human and AI)\n",
    "\n",
    "for index, chunk in enumerate(wd_list):\n",
    "\n",
    "    # Separate out the text columns we want for convenience\n",
    "    titles  = chunk[\"title\"]\n",
    "    bodies  = chunk[\"body\"]\n",
    "    answers = chunk[\"stackoverflow_answer\"]\n",
    "\n",
    "    # Create a new list to store HTML-stripped versions of text\n",
    "    s_gpt_answers  = []\n",
    "\n",
    "    # Iterating is slow(!) but comparatively easy to understand (modified Harvey approach)\n",
    "    for idx, *row in chunk.itertuples():\n",
    "        \n",
    "        # Strip HTML from GPT's answer (if it exists); save to list\n",
    "        if chunk[COL_NAME][idx] == None:\n",
    "            s_gpt_answers.append(None)\n",
    "        else:\n",
    "            s_gpt_answers.append(soup(chunk[COL_NAME][idx], \"html.parser\").get_text())\n",
    "\n",
    "    # Add the populated lists into our dataframe\n",
    "    stripped_col_name = \"stripped_\" + COL_NAME\n",
    "    chunk[stripped_col_name] = s_gpt_answers\n",
    "\n",
    "    # Preview result\n",
    "    #chunk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>accepted_answer_id</th>\n",
       "      <th>view_count</th>\n",
       "      <th>tags</th>\n",
       "      <th>answer_count</th>\n",
       "      <th>question_score</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>answer_score</th>\n",
       "      <th>stackoverflow_answer</th>\n",
       "      <th>stripped_title</th>\n",
       "      <th>stripped_body</th>\n",
       "      <th>stripped_stackoverflow_answer</th>\n",
       "      <th>gpt-4_answer</th>\n",
       "      <th>GPT_finished</th>\n",
       "      <th>full_GPT_response</th>\n",
       "      <th>stripped_gpt-4_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>408072</th>\n",
       "      <td>72674706</td>\n",
       "      <td>What is the REACT QUERY way to reuse global st...</td>\n",
       "      <td>&lt;p&gt;In redux/context api, we used to fetch the ...</td>\n",
       "      <td>72676239</td>\n",
       "      <td>9158</td>\n",
       "      <td>&lt;reactjs&gt;&lt;react-query&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-19 06:28:02</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;p&gt;You can just call &lt;code&gt;useQuery&lt;/code&gt; whe...</td>\n",
       "      <td>What is the REACT QUERY way to reuse global st...</td>\n",
       "      <td>In redux/context api, we used to fetch the dat...</td>\n",
       "      <td>You can just call useQuery wherever you want t...</td>\n",
       "      <td>React Query doesn't replace Redux or Context A...</td>\n",
       "      <td>True</td>\n",
       "      <td>{'id': 'chatcmpl-83Ku1F71nosfdcLHOuklckauIH42b...</td>\n",
       "      <td>React Query doesn't replace Redux or Context A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                              title  \\\n",
       "408072  72674706  What is the REACT QUERY way to reuse global st...   \n",
       "\n",
       "                                                     body  accepted_answer_id  \\\n",
       "408072  <p>In redux/context api, we used to fetch the ...            72676239   \n",
       "\n",
       "        view_count                    tags  answer_count  question_score  \\\n",
       "408072        9158  <reactjs><react-query>             1               1   \n",
       "\n",
       "             creation_date  answer_score  \\\n",
       "408072 2022-06-19 06:28:02             3   \n",
       "\n",
       "                                     stackoverflow_answer  \\\n",
       "408072  <p>You can just call <code>useQuery</code> whe...   \n",
       "\n",
       "                                           stripped_title  \\\n",
       "408072  What is the REACT QUERY way to reuse global st...   \n",
       "\n",
       "                                            stripped_body  \\\n",
       "408072  In redux/context api, we used to fetch the dat...   \n",
       "\n",
       "                            stripped_stackoverflow_answer  \\\n",
       "408072  You can just call useQuery wherever you want t...   \n",
       "\n",
       "                                             gpt-4_answer  GPT_finished  \\\n",
       "408072  React Query doesn't replace Redux or Context A...          True   \n",
       "\n",
       "                                        full_GPT_response  \\\n",
       "408072  {'id': 'chatcmpl-83Ku1F71nosfdcLHOuklckauIH42b...   \n",
       "\n",
       "                                    stripped_gpt-4_answer  \n",
       "408072  React Query doesn't replace Redux or Context A...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check first chunk\n",
    "wd_list[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>accepted_answer_id</th>\n",
       "      <th>view_count</th>\n",
       "      <th>tags</th>\n",
       "      <th>answer_count</th>\n",
       "      <th>question_score</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>answer_score</th>\n",
       "      <th>stackoverflow_answer</th>\n",
       "      <th>stripped_title</th>\n",
       "      <th>stripped_body</th>\n",
       "      <th>stripped_stackoverflow_answer</th>\n",
       "      <th>gpt-4_answer</th>\n",
       "      <th>GPT_finished</th>\n",
       "      <th>full_GPT_response</th>\n",
       "      <th>stripped_gpt-4_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>650846</th>\n",
       "      <td>74737897</td>\n",
       "      <td>Why can't make command see my Python venv dire...</td>\n",
       "      <td>&lt;p&gt;I'm writing a &lt;code&gt;Makefile&lt;/code&gt; for my ...</td>\n",
       "      <td>74738195</td>\n",
       "      <td>247</td>\n",
       "      <td>&lt;python&gt;&lt;makefile&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-12-09 01:03:11</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;p&gt;Per Enis's linked answer in the comments, t...</td>\n",
       "      <td>Why can't make command see my Python venv dire...</td>\n",
       "      <td>I'm writing a Makefile for my project (using M...</td>\n",
       "      <td>Per Enis's linked answer in the comments, this...</td>\n",
       "      <td>The issue here is that each line in a Makefile...</td>\n",
       "      <td>True</td>\n",
       "      <td>{'id': 'chatcmpl-83Ky7DD3JGyn09N8naXSKr7HImNjn...</td>\n",
       "      <td>The issue here is that each line in a Makefile...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                              title  \\\n",
       "650846  74737897  Why can't make command see my Python venv dire...   \n",
       "\n",
       "                                                     body  accepted_answer_id  \\\n",
       "650846  <p>I'm writing a <code>Makefile</code> for my ...            74738195   \n",
       "\n",
       "        view_count                tags  answer_count  question_score  \\\n",
       "650846         247  <python><makefile>             1               0   \n",
       "\n",
       "             creation_date  answer_score  \\\n",
       "650846 2022-12-09 01:03:11             0   \n",
       "\n",
       "                                     stackoverflow_answer  \\\n",
       "650846  <p>Per Enis's linked answer in the comments, t...   \n",
       "\n",
       "                                           stripped_title  \\\n",
       "650846  Why can't make command see my Python venv dire...   \n",
       "\n",
       "                                            stripped_body  \\\n",
       "650846  I'm writing a Makefile for my project (using M...   \n",
       "\n",
       "                            stripped_stackoverflow_answer  \\\n",
       "650846  Per Enis's linked answer in the comments, this...   \n",
       "\n",
       "                                             gpt-4_answer  GPT_finished  \\\n",
       "650846  The issue here is that each line in a Makefile...          True   \n",
       "\n",
       "                                        full_GPT_response  \\\n",
       "650846  {'id': 'chatcmpl-83Ky7DD3JGyn09N8naXSKr7HImNjn...   \n",
       "\n",
       "                                    stripped_gpt-4_answer  \n",
       "650846  The issue here is that each line in a Makefile...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check last chunk\n",
    "wd_list[9].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSONL file: c:\\Users\\Mark\\Documents\\A2I2 T2 2023\\eval_samples\\new_samples_0.jsonl\n",
      "Saved JSONL file: c:\\Users\\Mark\\Documents\\A2I2 T2 2023\\eval_samples\\new_samples_1.jsonl\n",
      "Saved JSONL file: c:\\Users\\Mark\\Documents\\A2I2 T2 2023\\eval_samples\\new_samples_2.jsonl\n",
      "Saved JSONL file: c:\\Users\\Mark\\Documents\\A2I2 T2 2023\\eval_samples\\new_samples_3.jsonl\n",
      "Saved JSONL file: c:\\Users\\Mark\\Documents\\A2I2 T2 2023\\eval_samples\\new_samples_4.jsonl\n",
      "Saved JSONL file: c:\\Users\\Mark\\Documents\\A2I2 T2 2023\\eval_samples\\new_samples_5.jsonl\n",
      "Saved JSONL file: c:\\Users\\Mark\\Documents\\A2I2 T2 2023\\eval_samples\\new_samples_6.jsonl\n",
      "Saved JSONL file: c:\\Users\\Mark\\Documents\\A2I2 T2 2023\\eval_samples\\new_samples_7.jsonl\n",
      "Saved JSONL file: c:\\Users\\Mark\\Documents\\A2I2 T2 2023\\eval_samples\\new_samples_8.jsonl\n",
      "Saved JSONL file: c:\\Users\\Mark\\Documents\\A2I2 T2 2023\\eval_samples\\new_samples_9.jsonl\n"
     ]
    }
   ],
   "source": [
    "for index, chunk in enumerate(wd_list):\n",
    "\n",
    "    json_list = []\n",
    "    for idx, row in chunk.iterrows():\n",
    "        \n",
    "        # Skip if the GPT answer is either unfinished or not present (e.g. token limit)\n",
    "        if (row[COL_NAME] == None) or (row[\"GPT_finished\"] == False):\n",
    "            continue\n",
    "\n",
    "        # Strip and chat-format the text as appropriate\n",
    "        title_and_question = row[\"stripped_title\"] + \"\\n\\n\" + row[\"stripped_body\"]\n",
    "        prompt = chat_format(title_and_question)\n",
    "\n",
    "        # Create the JSON object using the stripped and chat-formatted text\n",
    "        json_object = {\n",
    "            \"input\": prompt,\n",
    "            \"ideal\": row[\"stripped_stackoverflow_answer\"],\n",
    "            \"completion\": row[stripped_col_name]\n",
    "        }\n",
    "\n",
    "        # Add the object to our list\n",
    "        json_list.append(json.dumps(json_object))\n",
    "\n",
    "    # because re-running the notebook changes the sampling we DON'T want a persistent jsonl file\n",
    "    #   this step relies on the directories existing; they are not created here\n",
    "    #   so this will fail with e.g. FileNotFoundError if they don't exist\n",
    "    JSONL_FILENAME = f\"new_samples_{index}.jsonl\"\n",
    "    JSONL_FILEPATH = os.path.join(cwd, \"eval_samples\", JSONL_FILENAME)\n",
    "    with open(JSONL_FILEPATH, \"w\") as outfile:\n",
    "        outfile.write(\"\\n\".join(json_list))\n",
    "        print(\"Saved JSONL file: \" + JSONL_FILEPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install OpenAI evals\n",
    "\n",
    "As of writing, the version of evals on pip is wildly different from the version of evals available on GitHub (and even worse, they share the same version tag despite this). As such, even though we're not making our own eval we want the current version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'evals'...\n",
      "Updating files:  39% (474/1194)\n",
      "Updating files:  40% (478/1194)\n",
      "Updating files:  41% (490/1194)\n",
      "Updating files:  42% (502/1194)\n",
      "Updating files:  43% (514/1194)\n",
      "Updating files:  44% (526/1194)\n",
      "Updating files:  45% (538/1194)\n",
      "Updating files:  46% (550/1194)\n",
      "Updating files:  47% (562/1194)\n",
      "Updating files:  48% (574/1194)\n",
      "Updating files:  49% (586/1194)\n",
      "Updating files:  50% (597/1194)\n",
      "Updating files:  51% (609/1194)\n",
      "Updating files:  52% (621/1194)\n",
      "Updating files:  53% (633/1194)\n",
      "Updating files:  54% (645/1194)\n",
      "Updating files:  55% (657/1194)\n",
      "Updating files:  56% (669/1194)\n",
      "Updating files:  57% (681/1194)\n",
      "Updating files:  58% (693/1194)\n",
      "Updating files:  59% (705/1194)\n",
      "Updating files:  60% (717/1194)\n",
      "Updating files:  61% (729/1194)\n",
      "Updating files:  62% (741/1194)\n",
      "Updating files:  63% (753/1194)\n",
      "Updating files:  64% (765/1194)\n",
      "Updating files:  65% (777/1194)\n",
      "Updating files:  66% (789/1194)\n",
      "Updating files:  67% (800/1194)\n",
      "Updating files:  68% (812/1194)\n",
      "Updating files:  69% (824/1194)\n",
      "Updating files:  70% (836/1194)\n",
      "Updating files:  71% (848/1194)\n",
      "Updating files:  72% (860/1194)\n",
      "Updating files:  73% (872/1194)\n",
      "Updating files:  74% (884/1194)\n",
      "Updating files:  75% (896/1194)\n",
      "Updating files:  76% (908/1194)\n",
      "Updating files:  77% (920/1194)\n",
      "Updating files:  78% (932/1194)\n",
      "Updating files:  79% (944/1194)\n",
      "Updating files:  80% (956/1194)\n",
      "Updating files:  81% (968/1194)\n",
      "Updating files:  82% (980/1194)\n",
      "Updating files:  83% (992/1194)\n",
      "Updating files:  84% (1003/1194)\n",
      "Updating files:  85% (1015/1194)\n",
      "Updating files:  86% (1027/1194)\n",
      "Updating files:  87% (1039/1194)\n",
      "Updating files:  88% (1051/1194)\n",
      "Updating files:  89% (1063/1194)\n",
      "Updating files:  90% (1075/1194)\n",
      "Updating files:  91% (1087/1194)\n",
      "Updating files:  92% (1099/1194)\n",
      "Updating files:  93% (1111/1194)\n",
      "Updating files:  94% (1123/1194)\n",
      "Updating files:  95% (1135/1194)\n",
      "Updating files:  96% (1147/1194)\n",
      "Updating files:  97% (1159/1194)\n",
      "Updating files:  98% (1171/1194)\n",
      "Updating files:  99% (1183/1194)\n",
      "Updating files: 100% (1194/1194)\n",
      "Updating files: 100% (1194/1194), done.\n",
      "Filtering content:   0% (2/572)\n",
      "Filtering content:   1% (6/572)\n",
      "Filtering content:   2% (12/572)\n",
      "Filtering content:   3% (18/572), 22.01 MiB | 23.76 MiB/s\n",
      "Filtering content:   3% (19/572), 22.01 MiB | 23.76 MiB/s\n",
      "Filtering content:   4% (23/572), 22.01 MiB | 23.76 MiB/s\n",
      "Filtering content:   5% (29/572), 24.44 MiB | 13.24 MiB/s\n",
      "Filtering content:   6% (35/572), 24.44 MiB | 13.24 MiB/s\n",
      "Filtering content:   7% (41/572), 24.44 MiB | 13.24 MiB/s\n",
      "Filtering content:   7% (45/572), 26.03 MiB | 10.23 MiB/s\n",
      "Filtering content:   8% (46/572), 26.03 MiB | 10.23 MiB/s\n",
      "Filtering content:   9% (52/572), 26.03 MiB | 10.23 MiB/s\n",
      "Filtering content:  10% (58/572), 26.03 MiB | 10.23 MiB/s\n",
      "Filtering content:  11% (63/572), 26.55 MiB | 8.01 MiB/s \n",
      "Filtering content:  12% (69/572), 26.55 MiB | 8.01 MiB/s\n",
      "Filtering content:  12% (74/572), 26.55 MiB | 8.01 MiB/s\n",
      "Filtering content:  13% (75/572), 26.79 MiB | 6.55 MiB/s\n",
      "Filtering content:  14% (81/572), 26.79 MiB | 6.55 MiB/s\n",
      "Filtering content:  15% (86/572), 26.79 MiB | 6.55 MiB/s\n",
      "Filtering content:  16% (92/572), 26.92 MiB | 5.57 MiB/s\n",
      "Filtering content:  17% (98/572), 26.92 MiB | 5.57 MiB/s\n",
      "Filtering content:  17% (100/572), 26.92 MiB | 5.57 MiB/s\n",
      "Filtering content:  17% (101/572), 26.92 MiB | 5.57 MiB/s\n",
      "Filtering content:  18% (103/572), 28.20 MiB | 3.55 MiB/s\n",
      "Filtering content:  18% (105/572), 28.20 MiB | 3.55 MiB/s\n",
      "Filtering content:  19% (109/572), 37.31 MiB | 4.61 MiB/s\n",
      "Filtering content:  19% (112/572), 37.31 MiB | 4.61 MiB/s\n",
      "Filtering content:  20% (115/572), 50.19 MiB | 6.06 MiB/s\n",
      "Filtering content:  21% (121/572), 52.51 MiB | 4.39 MiB/s\n",
      "Filtering content:  21% (122/572), 52.51 MiB | 4.39 MiB/s\n",
      "Filtering content:  22% (126/572), 54.06 MiB | 4.33 MiB/s\n",
      "Filtering content:  23% (132/572), 54.88 MiB | 4.19 MiB/s\n",
      "Filtering content:  23% (133/572), 54.88 MiB | 4.19 MiB/s\n",
      "Filtering content:  24% (138/572), 55.35 MiB | 4.21 MiB/s\n",
      "Filtering content:  25% (143/572), 55.81 MiB | 4.27 MiB/s\n",
      "Filtering content:  25% (144/572), 55.81 MiB | 4.27 MiB/s\n",
      "Filtering content:  26% (149/572), 56.11 MiB | 4.28 MiB/s\n",
      "Filtering content:  27% (155/572), 56.37 MiB | 5.55 MiB/s\n",
      "Filtering content:  27% (156/572), 56.37 MiB | 5.55 MiB/s\n",
      "Filtering content:  28% (161/572), 137.16 MiB | 19.69 MiB/s\n",
      "Filtering content:  29% (166/572), 137.16 MiB | 19.69 MiB/s\n",
      "Filtering content:  30% (172/572), 137.16 MiB | 19.69 MiB/s\n",
      "Filtering content:  30% (175/572), 137.16 MiB | 19.69 MiB/s\n",
      "Filtering content:  31% (178/572), 137.47 MiB | 17.48 MiB/s\n",
      "Filtering content:  32% (184/572), 137.47 MiB | 17.48 MiB/s\n",
      "Filtering content:  33% (189/572), 137.58 MiB | 17.10 MiB/s\n",
      "Filtering content:  33% (194/572), 137.58 MiB | 17.10 MiB/s\n",
      "Filtering content:  34% (195/572), 137.58 MiB | 17.10 MiB/s\n",
      "Filtering content:  34% (200/572), 300.39 MiB | 42.71 MiB/s\n",
      "Filtering content:  35% (201/572), 300.39 MiB | 42.71 MiB/s\n",
      "Filtering content:  36% (206/572), 311.17 MiB | 37.24 MiB/s\n",
      "Filtering content:  37% (212/572), 348.55 MiB | 42.59 MiB/s\n",
      "Filtering content:  37% (217/572), 348.55 MiB | 42.59 MiB/s\n",
      "Filtering content:  38% (218/572), 348.55 MiB | 42.59 MiB/s\n",
      "Filtering content:  39% (224/572), 356.66 MiB | 43.92 MiB/s\n",
      "Filtering content:  40% (229/572), 356.66 MiB | 43.92 MiB/s\n",
      "Filtering content:  41% (235/572), 359.84 MiB | 44.02 MiB/s\n",
      "Filtering content:  42% (241/572), 359.84 MiB | 44.02 MiB/s\n",
      "Filtering content:  42% (242/572), 359.84 MiB | 44.02 MiB/s\n",
      "Filtering content:  43% (246/572), 359.84 MiB | 44.02 MiB/s\n",
      "Filtering content:  44% (252/572), 360.66 MiB | 33.24 MiB/s\n",
      "Filtering content:  45% (258/572), 360.66 MiB | 33.24 MiB/s\n",
      "Filtering content:  46% (264/572), 361.07 MiB | 33.26 MiB/s\n",
      "Filtering content:  47% (269/572), 361.07 MiB | 33.26 MiB/s\n",
      "Filtering content:  47% (271/572), 361.07 MiB | 33.26 MiB/s\n",
      "Filtering content:  48% (275/572), 361.07 MiB | 33.26 MiB/s\n",
      "Filtering content:  49% (281/572), 361.27 MiB | 33.32 MiB/s\n",
      "Filtering content:  50% (286/572), 361.27 MiB | 33.32 MiB/s\n",
      "Filtering content:  51% (292/572), 361.27 MiB | 33.32 MiB/s\n",
      "Filtering content:  52% (298/572), 361.39 MiB | 33.42 MiB/s\n",
      "Filtering content:  52% (301/572), 361.39 MiB | 33.42 MiB/s\n",
      "Filtering content:  52% (302/572), 366.39 MiB | 9.22 MiB/s \n",
      "Filtering content:  53% (304/572), 370.05 MiB | 9.74 MiB/s\n",
      "Filtering content:  54% (309/572), 370.05 MiB | 9.74 MiB/s\n",
      "Filtering content:  55% (315/572), 397.56 MiB | 7.95 MiB/s\n",
      "Filtering content:  55% (316/572), 397.56 MiB | 7.95 MiB/s\n",
      "Filtering content:  56% (321/572), 418.76 MiB | 10.12 MiB/s\n",
      "Filtering content:  57% (327/572), 418.76 MiB | 10.12 MiB/s\n",
      "Filtering content:  58% (332/572), 425.48 MiB | 10.80 MiB/s\n",
      "Filtering content:  58% (335/572), 425.48 MiB | 10.80 MiB/s\n",
      "Filtering content:  59% (338/572), 425.48 MiB | 10.80 MiB/s\n",
      "Filtering content:  60% (344/572), 427.08 MiB | 10.71 MiB/s\n",
      "Filtering content:  61% (349/572), 427.08 MiB | 10.71 MiB/s\n",
      "Filtering content:  62% (355/572), 427.08 MiB | 10.71 MiB/s\n",
      "Filtering content:  63% (361/572), 428.17 MiB | 10.76 MiB/s\n",
      "Filtering content:  63% (362/572), 428.17 MiB | 10.76 MiB/s\n",
      "Filtering content:  64% (367/572), 428.17 MiB | 10.76 MiB/s\n",
      "Filtering content:  65% (372/572), 428.17 MiB | 10.76 MiB/s\n",
      "Filtering content:  66% (378/572), 428.54 MiB | 10.80 MiB/s\n",
      "Filtering content:  67% (384/572), 428.54 MiB | 10.80 MiB/s\n",
      "Filtering content:  68% (389/572), 428.68 MiB | 10.89 MiB/s\n",
      "Filtering content:  68% (391/572), 428.68 MiB | 10.89 MiB/s\n",
      "Filtering content:  69% (395/572), 428.68 MiB | 10.89 MiB/s\n",
      "Filtering content:  70% (401/572), 428.68 MiB | 10.89 MiB/s\n",
      "Filtering content:  71% (407/572), 461.43 MiB | 11.65 MiB/s\n",
      "Filtering content:  71% (408/572), 461.43 MiB | 11.65 MiB/s\n",
      "Filtering content:  71% (411/572), 510.91 MiB | 14.43 MiB/s\n",
      "Filtering content:  72% (412/572), 524.22 MiB | 13.33 MiB/s\n",
      "Filtering content:  73% (418/572), 552.11 MiB | 15.95 MiB/s\n",
      "Filtering content:  74% (424/572), 552.11 MiB | 15.95 MiB/s\n",
      "Filtering content:  75% (429/572), 575.52 MiB | 18.73 MiB/s\n",
      "Filtering content:  76% (435/572), 575.52 MiB | 18.73 MiB/s\n",
      "Filtering content:  77% (441/572), 576.82 MiB | 18.83 MiB/s\n",
      "Filtering content:  77% (446/572), 576.82 MiB | 18.83 MiB/s\n",
      "Filtering content:  78% (447/572), 576.82 MiB | 18.83 MiB/s\n",
      "Filtering content:  79% (452/572), 576.82 MiB | 18.83 MiB/s\n",
      "Filtering content:  80% (458/572), 577.69 MiB | 18.86 MiB/s\n",
      "Filtering content:  81% (464/572), 577.69 MiB | 18.86 MiB/s\n",
      "Filtering content:  82% (470/572), 577.69 MiB | 18.86 MiB/s\n",
      "Filtering content:  83% (475/572), 577.97 MiB | 18.85 MiB/s\n",
      "Filtering content:  83% (477/572), 577.97 MiB | 18.85 MiB/s\n",
      "Filtering content:  84% (481/572), 577.97 MiB | 18.85 MiB/s\n",
      "Filtering content:  85% (487/572), 578.08 MiB | 28.04 MiB/s\n",
      "Filtering content:  86% (492/572), 578.08 MiB | 28.04 MiB/s\n",
      "Filtering content:  87% (498/572), 578.08 MiB | 28.04 MiB/s\n",
      "Filtering content:  87% (501/572), 578.14 MiB | 22.91 MiB/s\n",
      "Filtering content:  88% (504/572), 579.27 MiB | 11.30 MiB/s\n",
      "Filtering content:  88% (507/572), 579.27 MiB | 11.30 MiB/s\n",
      "Filtering content:  89% (510/572), 589.53 MiB | 10.88 MiB/s\n",
      "Filtering content:  90% (515/572), 589.53 MiB | 10.88 MiB/s\n",
      "Filtering content:  91% (521/572), 589.53 MiB | 10.88 MiB/s\n",
      "Filtering content:  92% (527/572), 607.81 MiB | 9.07 MiB/s \n",
      "Filtering content:  92% (530/572), 607.81 MiB | 9.07 MiB/s\n",
      "Filtering content:  93% (532/572), 607.81 MiB | 9.07 MiB/s\n",
      "Filtering content:  94% (538/572), 608.86 MiB | 5.52 MiB/s\n",
      "Filtering content:  95% (544/572), 608.86 MiB | 5.52 MiB/s\n",
      "Filtering content:  96% (550/572), 608.86 MiB | 5.52 MiB/s\n",
      "Filtering content:  97% (555/572), 609.32 MiB | 5.37 MiB/s\n",
      "Filtering content:  97% (558/572), 609.32 MiB | 5.37 MiB/s\n",
      "Filtering content:  98% (561/572), 609.32 MiB | 5.37 MiB/s\n",
      "Filtering content:  99% (567/572), 609.40 MiB | 5.27 MiB/s\n",
      "Filtering content: 100% (572/572), 609.40 MiB | 5.27 MiB/s\n",
      "Filtering content: 100% (572/572), 609.41 MiB | 14.92 MiB/s, done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetch: Fetching all references...\n",
      "Obtaining file:///C:/Users/Mark/Documents/A2I2%20T2%202023/evals\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Checking if build backend supports build_editable: started\n",
      "  Checking if build backend supports build_editable: finished with status 'done'\n",
      "  Getting requirements to build editable: started\n",
      "  Getting requirements to build editable: finished with status 'done'\n",
      "  Preparing editable metadata (pyproject.toml): started\n",
      "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: mypy in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (1.5.1)\n",
      "Requirement already satisfied: openai>=0.27.2 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (0.28.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (0.4.0)\n",
      "Requirement already satisfied: blobfile in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (2.0.2)\n",
      "Requirement already satisfied: backoff in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (2.2.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (1.25.2)\n",
      "Requirement already satisfied: snowflake-connector-python[pandas] in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (3.1.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (2.0.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (2.14.5)\n",
      "Requirement already satisfied: fire in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (0.5.0)\n",
      "Requirement already satisfied: pydantic in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (2.3.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (4.66.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (3.8.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (3.12.3)\n",
      "Requirement already satisfied: mock in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (5.1.0)\n",
      "Requirement already satisfied: langdetect in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (1.0.9)\n",
      "Requirement already satisfied: termcolor in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (2.3.0)\n",
      "Requirement already satisfied: lz4 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (4.3.2)\n",
      "Requirement already satisfied: pyzstd in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (0.15.9)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (6.0.1)\n",
      "Requirement already satisfied: sacrebleu in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (2.3.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (3.7.2)\n",
      "Requirement already satisfied: pytest in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (7.4.1)\n",
      "Requirement already satisfied: langchain in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (0.0.283)\n",
      "Requirement already satisfied: types-PyYAML in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from evals==1.0.3.post1) (6.0.12.11)\n",
      "Collecting spacy-universal-sentence-encoder (from evals==1.0.3.post1)\n",
      "  Using cached spacy_universal_sentence_encoder-0.4.6-py3-none-any.whl\n",
      "Collecting jiwer (from evals==1.0.3.post1)\n",
      "  Obtaining dependency information for jiwer from https://files.pythonhosted.org/packages/0d/4f/ee537ab20144811dd99321735ff92ef2b3a3230b77ed7454bed4c44d21fc/jiwer-3.0.3-py3-none-any.whl.metadata\n",
      "  Using cached jiwer-3.0.3-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from openai>=0.27.2->evals==1.0.3.post1) (2.31.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from openai>=0.27.2->evals==1.0.3.post1) (3.8.5)\n",
      "Requirement already satisfied: pycryptodomex~=3.8 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from blobfile->evals==1.0.3.post1) (3.18.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from blobfile->evals==1.0.3.post1) (1.26.16)\n",
      "Requirement already satisfied: lxml~=4.9 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from blobfile->evals==1.0.3.post1) (4.9.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from datasets->evals==1.0.3.post1) (10.0.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from datasets->evals==1.0.3.post1) (0.3.7)\n",
      "Requirement already satisfied: xxhash in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from datasets->evals==1.0.3.post1) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from datasets->evals==1.0.3.post1) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from datasets->evals==1.0.3.post1) (2023.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from datasets->evals==1.0.3.post1) (0.16.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from datasets->evals==1.0.3.post1) (23.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from tqdm->evals==1.0.3.post1) (0.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from fire->evals==1.0.3.post1) (1.16.0)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from jiwer->evals==1.0.3.post1) (8.1.7)\n",
      "Requirement already satisfied: rapidfuzz<4,>=3 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from jiwer->evals==1.0.3.post1) (3.3.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from langchain->evals==1.0.3.post1) (2.0.20)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from langchain->evals==1.0.3.post1) (0.5.14)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.21 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from langchain->evals==1.0.3.post1) (0.0.33)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from langchain->evals==1.0.3.post1) (2.8.5)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from langchain->evals==1.0.3.post1) (8.2.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from pydantic->evals==1.0.3.post1) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.6.3 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from pydantic->evals==1.0.3.post1) (2.6.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from pydantic->evals==1.0.3.post1) (4.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from matplotlib->evals==1.0.3.post1) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from matplotlib->evals==1.0.3.post1) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from matplotlib->evals==1.0.3.post1) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from matplotlib->evals==1.0.3.post1) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from matplotlib->evals==1.0.3.post1) (10.0.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from matplotlib->evals==1.0.3.post1) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from matplotlib->evals==1.0.3.post1) (2.8.2)\n",
      "Requirement already satisfied: mypy-extensions>=1.0.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from mypy->evals==1.0.3.post1) (1.0.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from nltk->evals==1.0.3.post1) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from nltk->evals==1.0.3.post1) (2023.8.8)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from pandas->evals==1.0.3.post1) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from pandas->evals==1.0.3.post1) (2023.3)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from pytest->evals==1.0.3.post1) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from pytest->evals==1.0.3.post1) (1.3.0)\n",
      "Requirement already satisfied: portalocker in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from sacrebleu->evals==1.0.3.post1) (2.7.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from sacrebleu->evals==1.0.3.post1) (0.9.0)\n",
      "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from snowflake-connector-python[pandas]->evals==1.0.3.post1) (1.5.1)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from snowflake-connector-python[pandas]->evals==1.0.3.post1) (1.15.1)\n",
      "Requirement already satisfied: cryptography<42.0.0,>=3.1.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from snowflake-connector-python[pandas]->evals==1.0.3.post1) (41.0.3)\n",
      "Requirement already satisfied: oscrypto<2.0.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from snowflake-connector-python[pandas]->evals==1.0.3.post1) (1.3.0)\n",
      "Requirement already satisfied: pyOpenSSL<24.0.0,>=16.2.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from snowflake-connector-python[pandas]->evals==1.0.3.post1) (23.2.0)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from snowflake-connector-python[pandas]->evals==1.0.3.post1) (2.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from snowflake-connector-python[pandas]->evals==1.0.3.post1) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from snowflake-connector-python[pandas]->evals==1.0.3.post1) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from snowflake-connector-python[pandas]->evals==1.0.3.post1) (2023.7.22)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from snowflake-connector-python[pandas]->evals==1.0.3.post1) (2.4.0)\n",
      "Collecting platformdirs<3.9.0,>=2.6.0 (from snowflake-connector-python[pandas]->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for platformdirs<3.9.0,>=2.6.0 from https://files.pythonhosted.org/packages/9e/d8/563a9fc17153c588c8c2042d2f0f84a89057cdb1c30270f589c88b42d62c/platformdirs-3.8.1-py3-none-any.whl.metadata\n",
      "  Using cached platformdirs-3.8.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: tomlkit in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from snowflake-connector-python[pandas]->evals==1.0.3.post1) (0.12.1)\n",
      "Collecting tensorflow<3.0.0,>=2.4.0 (from spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for tensorflow<3.0.0,>=2.4.0 from https://files.pythonhosted.org/packages/80/6f/57d36f6507e432d7fc1956b2e9e8530c5c2d2bfcd8821bcbfae271cd6688/tensorflow-2.14.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached tensorflow-2.14.0-cp311-cp311-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting spacy<4.0.0,>=3.0.0 (from spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for spacy<4.0.0,>=3.0.0 from https://files.pythonhosted.org/packages/d6/9e/8afc618cfed4b5dc602b11754d4d9193a268439704defae301bffca7f04c/spacy-3.6.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached spacy-3.6.1-cp311-cp311-win_amd64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: tensorflow-hub in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from spacy-universal-sentence-encoder->evals==1.0.3.post1) (0.14.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from aiohttp->openai>=0.27.2->evals==1.0.3.post1) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from aiohttp->openai>=0.27.2->evals==1.0.3.post1) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from aiohttp->openai>=0.27.2->evals==1.0.3.post1) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from aiohttp->openai>=0.27.2->evals==1.0.3.post1) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from aiohttp->openai>=0.27.2->evals==1.0.3.post1) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from aiohttp->openai>=0.27.2->evals==1.0.3.post1) (1.3.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python[pandas]->evals==1.0.3.post1) (2.21)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->evals==1.0.3.post1) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->evals==1.0.3.post1) (0.9.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (1.0.5)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for murmurhash<1.1.0,>=0.28.0 from https://files.pythonhosted.org/packages/71/46/af01a20ec368bd9cb49a1d2df15e3eca113bbf6952cc1f2a47f1c6801a7f/murmurhash-1.0.10-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached murmurhash-1.0.10-cp311-cp311-win_amd64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (2.0.8)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for preshed<3.1.0,>=3.0.2 from https://files.pythonhosted.org/packages/e4/fc/78cdbdb79f5d6d45949e72c32445d6c060977ad50a1dcfc0392622165f7c/preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.2.0,>=8.1.8 (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for thinc<8.2.0,>=8.1.8 from https://files.pythonhosted.org/packages/ea/65/9fe6fe1ddb5fd34b7b81dada121e6862791e624384a2964331d0228aea38/thinc-8.1.12-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached thinc-8.1.12-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (1.1.2)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for srsly<3.0.0,>=2.4.3 from https://files.pythonhosted.org/packages/eb/f5/e3f29993f673d91623df6413ba64e815dd2676fd7932cbc5e7347402ddae/srsly-2.4.8-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached srsly-2.4.8-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for catalogue<2.1.0,>=2.0.6 from https://files.pythonhosted.org/packages/9e/96/d32b941a501ab566a16358d68b6eb4e4acc373fab3c3c4d7d9e649f7b4bb/catalogue-2.0.10-py3-none-any.whl.metadata\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Using cached typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Collecting pathy>=0.10.0 (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for pathy>=0.10.0 from https://files.pythonhosted.org/packages/b5/c3/04a002ace658133f5ac48d30258ed9ceab720595dc1ac36df02fe52018af/pathy-0.10.2-py3-none-any.whl.metadata\n",
      "  Using cached pathy-0.10.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (6.4.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (68.0.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain->evals==1.0.3.post1) (2.0.2)\n",
      "Collecting tensorflow-intel==2.14.0 (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for tensorflow-intel==2.14.0 from https://files.pythonhosted.org/packages/ad/6e/1bfe367855dd87467564f7bf9fa14f3b17889988e79598bc37bf18f5ffb6/tensorflow_intel-2.14.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached tensorflow_intel-2.14.0-cp311-cp311-win_amd64.whl.metadata (4.8 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for absl-py>=1.0.0 from https://files.pythonhosted.org/packages/01/e4/dc0a1dcc4e74e08d7abedab278c795eef54a224363bb18f5692f416d834f/absl_py-2.0.0-py3-none-any.whl.metadata\n",
      "  Using cached absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (23.5.26)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Using cached gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting h5py>=2.9.0 (from tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for h5py>=2.9.0 from https://files.pythonhosted.org/packages/d1/93/0f4cf5058095d749d464e4f770d2bf339930e5f3374331f0d2fa6ddfbf28/h5py-3.9.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached h5py-3.9.0-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (16.0.6)\n",
      "Collecting ml-dtypes==0.2.0 (from tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for ml-dtypes==0.2.0 from https://files.pythonhosted.org/packages/08/89/c727fde1a3d12586e0b8c01abf53754707d76beaa9987640e70807d4545f/ml_dtypes-0.2.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached ml_dtypes-0.2.0-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (4.24.2)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (1.57.0)\n",
      "Collecting tensorboard<2.15,>=2.14 (from tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for tensorboard<2.15,>=2.14 from https://files.pythonhosted.org/packages/bc/a2/ff5f4c299eb37c95299a76015da3f30211468e29d8d6f1d011683279baee/tensorboard-2.14.0-py3-none-any.whl.metadata\n",
      "  Using cached tensorboard-2.14.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (2.14.0)\n",
      "Collecting keras<2.15,>=2.14.0 (from tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for keras<2.15,>=2.14.0 from https://files.pythonhosted.org/packages/fe/58/34d4d8f1aa11120c2d36d7ad27d0526164b1a8ae45990a2fede31d0e59bf/keras-2.14.0-py3-none-any.whl.metadata\n",
      "  Using cached keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from portalocker->sacrebleu->evals==1.0.3.post1) (305.1)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for blis<0.8.0,>=0.7.8 from https://files.pythonhosted.org/packages/2f/09/da0592c74560cc33396504698122f7a56747c82a5e072ca7d2c3397898e1/blis-0.7.11-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached blis-0.7.11-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for confection<1.0.0,>=0.0.1 from https://files.pythonhosted.org/packages/93/f8/e89268a1f885048fb2ee6b5c9f93c4e90de768534acfef3652f87d97d4cb/confection-0.1.3-py3-none-any.whl.metadata\n",
      "  Using cached confection-0.1.3-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from jinja2->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (2.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (2.22.0)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Obtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/1a/b5/228c1cdcfe138f1a8e01ab1b54284c8b83735476cb22b6ba251656ed13ad/Markdown-3.4.4-py3-none-any.whl.metadata\n",
      "  Using cached Markdown-3.4.4-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (2.3.7)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (4.9)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\mark\\anaconda3\\envs\\testenv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1) (0.5.0)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals==1.0.3.post1)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached jiwer-3.0.3-py3-none-any.whl (21 kB)\n",
      "Using cached platformdirs-3.8.1-py3-none-any.whl (16 kB)\n",
      "Using cached spacy-3.6.1-cp311-cp311-win_amd64.whl (12.0 MB)\n",
      "Using cached tensorflow-2.14.0-cp311-cp311-win_amd64.whl (2.1 kB)\n",
      "Using cached tensorflow_intel-2.14.0-cp311-cp311-win_amd64.whl (284.2 MB)\n",
      "Using cached ml_dtypes-0.2.0-cp311-cp311-win_amd64.whl (938 kB)\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Using cached murmurhash-1.0.10-cp311-cp311-win_amd64.whl (25 kB)\n",
      "Using cached pathy-0.10.2-py3-none-any.whl (48 kB)\n",
      "Using cached preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
      "Using cached srsly-2.4.8-cp311-cp311-win_amd64.whl (479 kB)\n",
      "Using cached thinc-8.1.12-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "Using cached absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "Using cached blis-0.7.11-cp311-cp311-win_amd64.whl (6.6 MB)\n",
      "Using cached confection-0.1.3-py3-none-any.whl (34 kB)\n",
      "Using cached h5py-3.9.0-cp311-cp311-win_amd64.whl (2.7 MB)\n",
      "Using cached keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
      "Using cached tensorboard-2.14.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "Building wheels for collected packages: evals\n",
      "  Building editable for evals (pyproject.toml): started\n",
      "  Building editable for evals (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for evals: filename=evals-1.0.3.post1-0.editable-py3-none-any.whl size=4971 sha256=bd4c1ccb48b9234b048c880413d690d31a1b08a7503612593303cd7e34290ca9\n",
      "  Stored in directory: C:\\Users\\Mark\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-naww2ss1\\wheels\\21\\9b\\5f\\955a0fd905ea424144a3217ef64f9aacf3ceb73381d6f29710\n",
      "Successfully built evals\n",
      "Installing collected packages: platformdirs, opt-einsum, oauthlib, murmurhash, ml-dtypes, markdown, langcodes, keras, h5py, google-pasta, gast, catalogue, blis, astunparse, absl-py, typer, srsly, requests-oauthlib, preshed, jiwer, pathy, google-auth-oauthlib, confection, thinc, tensorboard, tensorflow-intel, spacy, tensorflow, spacy-universal-sentence-encoder, evals\n",
      "  Attempting uninstall: platformdirs\n",
      "    Found existing installation: platformdirs 3.10.0\n",
      "    Uninstalling platformdirs-3.10.0:\n",
      "      Successfully uninstalled platformdirs-3.10.0\n",
      "  Attempting uninstall: evals\n",
      "    Found existing installation: evals 1.0.3.post1\n",
      "    Uninstalling evals-1.0.3.post1:\n",
      "      Successfully uninstalled evals-1.0.3.post1\n",
      "Successfully installed absl-py-2.0.0 astunparse-1.6.3 blis-0.7.11 catalogue-2.0.10 confection-0.1.3 evals-1.0.3.post1 gast-0.5.4 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 h5py-3.9.0 jiwer-3.0.3 keras-2.14.0 langcodes-3.3.0 markdown-3.4.4 ml-dtypes-0.2.0 murmurhash-1.0.10 oauthlib-3.2.2 opt-einsum-3.3.0 pathy-0.10.2 platformdirs-3.8.1 preshed-3.0.9 requests-oauthlib-1.3.1 spacy-3.6.1 spacy-universal-sentence-encoder-0.4.6 srsly-2.4.8 tensorboard-2.14.0 tensorflow-2.14.0 tensorflow-intel-2.14.0 thinc-8.1.12 typer-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# openai evals uses git-lfs, but installation may be system-specific\n",
    "#!git lfs install\n",
    "\n",
    "# get a local copy of evals (and if one already exists, nuke it first)\n",
    "try:\n",
    "    !rm -rf evals\n",
    "finally:\n",
    "    !git clone https://github.com/MHLoppy/evals.git\n",
    "\n",
    "# complete the remaining setup steps\n",
    "!cd evals\n",
    "!git lfs fetch --all\n",
    "!git lfs pull\n",
    "%pip install -e evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use/Run evals\n",
    "\n",
    "(As an aside, while using evals without both magic commands and manual file creation is possible (see https://medium.com/@sergioli/evaluating-chatgpt-using-openai-evals-7ca85c0ad139), it's comparatively more complex.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note how the lists here are appended with an extra set of quotes\n",
    "#   this is being done because we're running shell commands that need quotes\n",
    "\n",
    "# Construct list of sample file paths\n",
    "sample_paths = []\n",
    "for index, chunk in enumerate(wd_list):\n",
    "    sample_path = os.path.join(cwd, \"eval_samples\", f\"new_samples_{index}.jsonl\")\n",
    "    sample_paths.append(f\"{sample_path}\")\n",
    "\n",
    "# Construct list of record file paths\n",
    "record_paths = []\n",
    "for index, chunk in enumerate(wd_list):\n",
    "    record_path = os.path.join(cwd, \"eval_records\", f\"eval_record_{index}.jsonl\")\n",
    "    record_paths.append(f'\\\"{record_path}\\\"')\n",
    "\n",
    "# Construct list of log file paths\n",
    "log_paths = []\n",
    "for index, chunk in enumerate(wd_list):\n",
    "    log_path = os.path.join(cwd, \"eval_logs\", f\"eval_log_{index}.jsonl\")\n",
    "    log_paths.append(f'\\\"{log_path}\\\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:31<00:00, 31.97s/it]\n",
      "100%|██████████| 1/1 [00:31<00:00, 31.97s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:10<00:00, 10.55s/it]\n",
      "100%|██████████| 1/1 [00:10<00:00, 10.55s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:23<00:00, 23.09s/it]\n",
      "100%|██████████| 1/1 [00:23<00:00, 23.09s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:11<00:00, 11.18s/it]\n",
      "100%|██████████| 1/1 [00:11<00:00, 11.18s/it]\n",
      "\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:15<00:00, 15.50s/it]\n",
      "100%|██████████| 1/1 [00:15<00:00, 15.50s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:14<00:00, 14.04s/it]\n",
      "100%|██████████| 1/1 [00:14<00:00, 14.04s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:14<00:00, 14.41s/it]\n",
      "100%|██████████| 1/1 [00:14<00:00, 14.41s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:17<00:00, 17.87s/it]\n",
      "100%|██████████| 1/1 [00:17<00:00, 17.87s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:24<00:00, 24.47s/it]\n",
      "100%|██████████| 1/1 [00:24<00:00, 24.47s/it]\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "samples_file_path = os.path.join(cwd, \"evals\", \"evals\", \"registry\", \"data\", \"coqa\", \"samples.jsonl\")\n",
    "\n",
    "# Run chunked evals\n",
    "for index, chunk in enumerate(wd_list):\n",
    "\n",
    "    # Update the samples file programmatically each iteration\n",
    "    shutil.copy(sample_paths[index], samples_file_path)\n",
    "\n",
    "    # Run the evaluation and save the results (records) and log file as specified\n",
    "    record = record_paths[index]\n",
    "    log = log_paths[index]\n",
    "    !oaieval gpt-4 coqa-fact --record_path $record --log_to_file $log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 records were skipped because they did not seem to contain a completed evaluation.\n"
     ]
    }
   ],
   "source": [
    "# obed's answer reading code\n",
    "#   (because by his evaluation, the metric disagrees with the written response ~10% of the time)\n",
    "import re\n",
    "def get_answer_from_response(text):\n",
    "    \"\"\"Parses the output text for the evaluation choice.\"\"\"\n",
    "    \n",
    "    last_letter = text[-1]\n",
    "    if last_letter not in ['A', 'B', 'C', 'D', 'E']:\n",
    "        matches = re.findall('\\((.*?)\\)', text)\n",
    "        return matches[-1] if matches else None      \n",
    "    return last_letter\n",
    "\n",
    "# Construct list of record file paths WITHOUT extra quotes\n",
    "record_paths_nq = []\n",
    "for index, chunk in enumerate(wd_list):\n",
    "    record_path = os.path.join(cwd, \"eval_records\", f\"eval_record_{index}.jsonl\")\n",
    "    record_paths_nq.append(f\"{record_path}\")\n",
    "\n",
    "# Track records skipped due to seemingly missing evaluations\n",
    "skipped_records = 0\n",
    "\n",
    "# Iterate through our evals results\n",
    "for index, chunk in enumerate(wd_list):\n",
    "\n",
    "    # Update our record path each iteration\n",
    "    record_path = record_paths_nq[index]\n",
    "\n",
    "    # Empty lists for us to store stuff in, to later add to the chunk's dataframe\n",
    "    answer_list = []\n",
    "    provided_answer_list = []\n",
    "    sampled_list = []\n",
    "    metric_list = []\n",
    "\n",
    "    # If file is too small, it SURELY does NOT contain evaluations\n",
    "    # (I swear I first tried all the json-parsing ways of doing this I could think of)\n",
    "    if os.path.getsize(record_path) > 1500:\n",
    "\n",
    "        # Open the record (.jsonl) for this iteration\n",
    "        with open(record_path) as f:\n",
    "\n",
    "            # Skip the first two lines (they're just metadata about the query and response)\n",
    "            for _ in range(2):\n",
    "                next(f)\n",
    "\n",
    "            # Iterate through the rest of the file line-by-line\n",
    "            for line in f:\n",
    "                eval_line = json.loads(line)\n",
    "\n",
    "                # process the \"sampling\" half of each evaluation response\n",
    "                if eval_line[\"type\"] == \"sampling\":\n",
    "\n",
    "                    # Extract the response from the answer received\n",
    "                    answer = eval_line[\"data\"][\"sampled\"][0]\n",
    "                    extr_choice = get_answer_from_response(answer)\n",
    "\n",
    "                    # Add the extracted response and the raw response to our lists\n",
    "                    answer_list.append(extr_choice)\n",
    "                    sampled_list.append(eval_line)\n",
    "\n",
    "                # Process the \"metric\" half of each evaluation response\n",
    "                elif eval_line[\"type\"] == \"metrics\":\n",
    "\n",
    "                    # Also pull evals' self-reported response\n",
    "                    og_choice = eval_line[\"data\"][\"choice\"]\n",
    "\n",
    "                    # Add that and the raw response to our lists\n",
    "                    provided_answer_list.append(og_choice)\n",
    "                    metric_list.append(eval_line)\n",
    "    \n",
    "    # The file is suspiciously small, so probably does not contain any completed evaluations          \n",
    "    else:\n",
    "        answer_list.append(None)\n",
    "        provided_answer_list.append(None)\n",
    "        sampled_list.append(None)\n",
    "        metric_list.append(None)\n",
    "        skipped_records += 1\n",
    "\n",
    "    # Add the populated lists into our chunk's dataframe\n",
    "    chunk[\"original_eval_choice\"] = provided_answer_list\n",
    "    chunk[\"extracted_eval_choice\"] = answer_list\n",
    "    chunk[\"eval_full_sampled\"] = sampled_list\n",
    "    chunk[\"eval_full_metric\"] = metric_list\n",
    "\n",
    "print(skipped_records, \"records were skipped because they did not seem to contain a completed evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>accepted_answer_id</th>\n",
       "      <th>view_count</th>\n",
       "      <th>tags</th>\n",
       "      <th>answer_count</th>\n",
       "      <th>question_score</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>answer_score</th>\n",
       "      <th>...</th>\n",
       "      <th>stripped_body</th>\n",
       "      <th>stripped_stackoverflow_answer</th>\n",
       "      <th>gpt-4_answer</th>\n",
       "      <th>GPT_finished</th>\n",
       "      <th>full_GPT_response</th>\n",
       "      <th>stripped_gpt-4_answer</th>\n",
       "      <th>original_eval_choice</th>\n",
       "      <th>extracted_eval_choice</th>\n",
       "      <th>eval_full_sampled</th>\n",
       "      <th>eval_full_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>408072</th>\n",
       "      <td>72674706</td>\n",
       "      <td>What is the REACT QUERY way to reuse global st...</td>\n",
       "      <td>&lt;p&gt;In redux/context api, we used to fetch the ...</td>\n",
       "      <td>72676239</td>\n",
       "      <td>9158</td>\n",
       "      <td>&lt;reactjs&gt;&lt;react-query&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-19 06:28:02</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>In redux/context api, we used to fetch the dat...</td>\n",
       "      <td>You can just call useQuery wherever you want t...</td>\n",
       "      <td>React Query doesn't replace Redux or Context A...</td>\n",
       "      <td>True</td>\n",
       "      <td>{'id': 'chatcmpl-83Ku1F71nosfdcLHOuklckauIH42b...</td>\n",
       "      <td>React Query doesn't replace Redux or Context A...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>{'run_id': '230927103501KYBTZMP5', 'event_id':...</td>\n",
       "      <td>{'run_id': '230927103501KYBTZMP5', 'event_id':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619720</th>\n",
       "      <td>74458975</td>\n",
       "      <td>how to make Localization with Getx if there is...</td>\n",
       "      <td>&lt;p&gt;if the text like this:&lt;/p&gt;&amp;#xA;&lt;pre&gt;&lt;code&gt; ...</td>\n",
       "      <td>74459510</td>\n",
       "      <td>2019</td>\n",
       "      <td>&lt;flutter&gt;&lt;flutter-getx&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-11-16 10:36:18</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>if the text like this:\\n           Text(\\n    ...</td>\n",
       "      <td>The documentation of GetX explains well how yo...</td>\n",
       "      <td>Yes, you can use parameters in Getx localizati...</td>\n",
       "      <td>True</td>\n",
       "      <td>{'id': 'chatcmpl-83KuVZo7dTedZojkpqNUSFbUQQglO...</td>\n",
       "      <td>Yes, you can use parameters in Getx localizati...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>{'run_id': '230927103535C2AWOIYP', 'event_id':...</td>\n",
       "      <td>{'run_id': '230927103535C2AWOIYP', 'event_id':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693780</th>\n",
       "      <td>75121109</td>\n",
       "      <td>React Query invalidateQueries not updating the UI</td>\n",
       "      <td>&lt;p&gt;My UI is not updating on the creation of a ...</td>\n",
       "      <td>75122016</td>\n",
       "      <td>492</td>\n",
       "      <td>&lt;reactjs&gt;&lt;next.js&gt;&lt;react-query&gt;&lt;supabase&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-14 20:44:38</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>My UI is not updating on the creation of a pro...</td>\n",
       "      <td>You are instantiating QueryClient on every ren...</td>\n",
       "      <td>The issue might be due to the stale data in th...</td>\n",
       "      <td>True</td>\n",
       "      <td>{'id': 'chatcmpl-83KuwQ6AgW7kFy4JPR9slz59xvYK1...</td>\n",
       "      <td>The issue might be due to the stale data in th...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>{'run_id': '230927103548ZLOAKU7X', 'event_id':...</td>\n",
       "      <td>{'run_id': '230927103548ZLOAKU7X', 'event_id':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118789</th>\n",
       "      <td>70326274</td>\n",
       "      <td>jQuery find empty inputs for particular form</td>\n",
       "      <td>&lt;p&gt;I am trying to find and set values on empty...</td>\n",
       "      <td>70326361</td>\n",
       "      <td>22</td>\n",
       "      <td>&lt;jquery&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-12-12 18:01:41</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>I am trying to find and set values on empty fo...</td>\n",
       "      <td>You need to look inside the form instance usin...</td>\n",
       "      <td>You can use the `find` method to search for el...</td>\n",
       "      <td>True</td>\n",
       "      <td>{'id': 'chatcmpl-83KvV2AvQSeaPEswfLfvYCXbWWvF6...</td>\n",
       "      <td>You can use the `find` method to search for el...</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>{'run_id': '230927103613GAUEEG47', 'event_id':...</td>\n",
       "      <td>{'run_id': '230927103613GAUEEG47', 'event_id':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66379</th>\n",
       "      <td>69913488</td>\n",
       "      <td>How to authenticate a ldap user from any ldap ...</td>\n",
       "      <td>&lt;p&gt;I want to implement ldap authentication in ...</td>\n",
       "      <td>70022969</td>\n",
       "      <td>1952</td>\n",
       "      <td>&lt;spring-boot&gt;&lt;spring-security&gt;&lt;active-director...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-11-10 12:44:34</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>I want to implement ldap authentication in an ...</td>\n",
       "      <td>OK. So after spending lot of times I got solut...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                              title  \\\n",
       "408072  72674706  What is the REACT QUERY way to reuse global st...   \n",
       "619720  74458975  how to make Localization with Getx if there is...   \n",
       "693780  75121109  React Query invalidateQueries not updating the UI   \n",
       "118789  70326274       jQuery find empty inputs for particular form   \n",
       "66379   69913488  How to authenticate a ldap user from any ldap ...   \n",
       "\n",
       "                                                     body  accepted_answer_id  \\\n",
       "408072  <p>In redux/context api, we used to fetch the ...            72676239   \n",
       "619720  <p>if the text like this:</p>&#xA;<pre><code> ...            74459510   \n",
       "693780  <p>My UI is not updating on the creation of a ...            75122016   \n",
       "118789  <p>I am trying to find and set values on empty...            70326361   \n",
       "66379   <p>I want to implement ldap authentication in ...            70022969   \n",
       "\n",
       "        view_count                                               tags  \\\n",
       "408072        9158                             <reactjs><react-query>   \n",
       "619720        2019                            <flutter><flutter-getx>   \n",
       "693780         492          <reactjs><next.js><react-query><supabase>   \n",
       "118789          22                                           <jquery>   \n",
       "66379         1952  <spring-boot><spring-security><active-director...   \n",
       "\n",
       "        answer_count  question_score       creation_date  answer_score  ...  \\\n",
       "408072             1               1 2022-06-19 06:28:02             3  ...   \n",
       "619720             1               2 2022-11-16 10:36:18            10  ...   \n",
       "693780             1               1 2023-01-14 20:44:38             2  ...   \n",
       "118789             1               0 2021-12-12 18:01:41             0  ...   \n",
       "66379              1               1 2021-11-10 12:44:34             0  ...   \n",
       "\n",
       "                                            stripped_body  \\\n",
       "408072  In redux/context api, we used to fetch the dat...   \n",
       "619720  if the text like this:\\n           Text(\\n    ...   \n",
       "693780  My UI is not updating on the creation of a pro...   \n",
       "118789  I am trying to find and set values on empty fo...   \n",
       "66379   I want to implement ldap authentication in an ...   \n",
       "\n",
       "                            stripped_stackoverflow_answer  \\\n",
       "408072  You can just call useQuery wherever you want t...   \n",
       "619720  The documentation of GetX explains well how yo...   \n",
       "693780  You are instantiating QueryClient on every ren...   \n",
       "118789  You need to look inside the form instance usin...   \n",
       "66379   OK. So after spending lot of times I got solut...   \n",
       "\n",
       "                                             gpt-4_answer GPT_finished  \\\n",
       "408072  React Query doesn't replace Redux or Context A...         True   \n",
       "619720  Yes, you can use parameters in Getx localizati...         True   \n",
       "693780  The issue might be due to the stale data in th...         True   \n",
       "118789  You can use the `find` method to search for el...         True   \n",
       "66379                                                None        False   \n",
       "\n",
       "                                        full_GPT_response  \\\n",
       "408072  {'id': 'chatcmpl-83Ku1F71nosfdcLHOuklckauIH42b...   \n",
       "619720  {'id': 'chatcmpl-83KuVZo7dTedZojkpqNUSFbUQQglO...   \n",
       "693780  {'id': 'chatcmpl-83KuwQ6AgW7kFy4JPR9slz59xvYK1...   \n",
       "118789  {'id': 'chatcmpl-83KvV2AvQSeaPEswfLfvYCXbWWvF6...   \n",
       "66379                                                None   \n",
       "\n",
       "                                    stripped_gpt-4_answer  \\\n",
       "408072  React Query doesn't replace Redux or Context A...   \n",
       "619720  Yes, you can use parameters in Getx localizati...   \n",
       "693780  The issue might be due to the stale data in th...   \n",
       "118789  You can use the `find` method to search for el...   \n",
       "66379                                                None   \n",
       "\n",
       "       original_eval_choice extracted_eval_choice  \\\n",
       "408072                    D                     D   \n",
       "619720                    D                     D   \n",
       "693780                    D                     D   \n",
       "118789                    B                     B   \n",
       "66379                  None                  None   \n",
       "\n",
       "                                        eval_full_sampled  \\\n",
       "408072  {'run_id': '230927103501KYBTZMP5', 'event_id':...   \n",
       "619720  {'run_id': '230927103535C2AWOIYP', 'event_id':...   \n",
       "693780  {'run_id': '230927103548ZLOAKU7X', 'event_id':...   \n",
       "118789  {'run_id': '230927103613GAUEEG47', 'event_id':...   \n",
       "66379                                                None   \n",
       "\n",
       "                                         eval_full_metric  \n",
       "408072  {'run_id': '230927103501KYBTZMP5', 'event_id':...  \n",
       "619720  {'run_id': '230927103535C2AWOIYP', 'event_id':...  \n",
       "693780  {'run_id': '230927103548ZLOAKU7X', 'event_id':...  \n",
       "118789  {'run_id': '230927103613GAUEEG47', 'event_id':...  \n",
       "66379                                                None  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine each of the chunked dataframes, export to CSV\n",
    "combined = pd.concat(wd_list)\n",
    "combined.to_csv(\"dataset_results.csv\")\n",
    "\n",
    "# Preview the result\n",
    "combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra bit for estimating the token count of the evals prompt (since this will count against our token limit!).\n",
    "\n",
    "This cell is not part of the rest of the data processing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243\n"
     ]
    }
   ],
   "source": [
    "# Estimate token count of evals prompt\n",
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "text = \"\\n\\nCompare the factual content of the submitted answer with the expert answer. Ignore any differences in style, grammar, or punctuation.\\nThe submitted answer may either be a subset or superset of the expert answer, or it may conflict with it. Determine which case applies. Answer the question by selecting one of the following options:\\n(A) The submitted answer is a subset of the expert answer and is fully consistent with it.\\n(B) The submitted answer is a superset of the expert answer and is fully consistent with it.\\n(C) The submitted answer contains all the same details as the expert answer.\\n(D) There is a disagreement between the submitted answer and the expert answer.\\n(E) The answers differ, but these differences don't matter from the perspective of factuality.\\n\\nFirst, write out in a step by step manner your reasoning to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset. Then print only a single choice from \\\"A\\\" or \\\"B\\\" or \\\"C\\\" or \\\"D\\\" or \\\"E\\\" (without quotes or punctuation) on its own line corresponding to the correct answer. At the end, repeat just the answer by itself on a new line.\\n\\nReasoning:\"\n",
    "\n",
    "tokens = len(encoding.encode(text, disallowed_special=()))\n",
    "\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
